{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-18T06:43:51.595812Z",
     "start_time": "2024-04-18T06:43:51.592635Z"
    }
   },
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn import Sequential, Flatten, Linear, Module, Conv2d, MaxPool2d, CrossEntropyLoss, Tanh, ReLU, Softmax, \\\n",
    "    BatchNorm2d, Dropout\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T06:30:32.745184Z",
     "start_time": "2024-04-18T06:30:32.723024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Device is set to: {device}\")"
   ],
   "id": "561996276d71f643",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is set to: mps\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T06:39:32.032322Z",
     "start_time": "2024-04-18T06:30:32.745729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = DataLoader(\n",
    "    datasets.CIFAR10(root=\"data\", train=True, download=True, transform=ToTensor()))\n",
    "X, _ = next(iter(data))\n",
    "mean, std = X.mean(), X.std()\n",
    "\n",
    "training_data = datasets.CIFAR10(root=\"data\", train=True, download=True,\n",
    "                                 transform=Compose([ToTensor(), Normalize(mean, std)]))\n",
    "test_data = datasets.CIFAR10(root=\"data\", train=False, download=True,\n",
    "                             transform=Compose([ToTensor(), Normalize(mean, std)]))\n",
    "print(f'Mean: {mean}, Std: {std}')\n",
    "print(f'Training data: {len(training_data)}')\n",
    "print(f'Test data: {len(test_data)}')\n",
    "print(f'Test data shape: {test_data.data.shape}')"
   ],
   "id": "76b1703d6b6d7b1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Mean: 0.4056755602359772, Std: 0.20385168492794037\n",
      "Training data: 50000\n",
      "Test data: 10000\n",
      "Test data shape: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T14:35:23.490950Z",
     "start_time": "2024-04-18T14:35:23.437205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EmptyForward(Module):\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return x\n",
    "\n",
    "class PrintForward(Module):\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "class CNN(Module):\n",
    "    def __init__(self, n_classes=10, n_channels=3, activation_function=ReLU(), output_activation=Softmax(),\n",
    "                 batch_normalization=False, dropout_rate=0):\n",
    "        super(CNN, self).__init__()\n",
    "        self.model = Sequential(\n",
    "            Conv2d(1, 32, 3, 1),  # 32 * 32 * 32\n",
    "            BatchNorm2d(32) if batch_normalization else EmptyForward(),\n",
    "            activation_function,\n",
    "            Conv2d(32, 64, 3, 1),  # 64 * 31 * 31\n",
    "            BatchNorm2d(64) if batch_normalization else EmptyForward(),\n",
    "            activation_function,\n",
    "            MaxPool2d(2, 2),  # 64 * 15 * 15\n",
    "            Conv2d(64, 128, 3, 1),  # 128 * 14 * 14\n",
    "            BatchNorm2d(128) if batch_normalization else EmptyForward(),\n",
    "            activation_function,\n",
    "            Conv2d(128, 128, 3, 1),  # 128 * 13 * 13\n",
    "            BatchNorm2d(128) if batch_normalization else EmptyForward(),\n",
    "            Flatten(),\n",
    "            Dropout(dropout_rate),\n",
    "            PrintForward(),\n",
    "            Linear(128 * 13 * 13, 128),\n",
    "            BatchNorm2d(64) if batch_normalization else EmptyForward(),\n",
    "            activation_function,\n",
    "            Dropout(dropout_rate),\n",
    "            Linear(128, 64),\n",
    "            BatchNorm2d(32) if batch_normalization else EmptyForward(),\n",
    "            activation_function,\n",
    "            Dropout(dropout_rate),\n",
    "            Linear(64, 10),\n",
    "            BatchNorm2d(10) if batch_normalization else EmptyForward(),\n",
    "            output_activation\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "id": "7295e4c6e6db9f17",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T14:35:24.310491Z",
     "start_time": "2024-04-18T14:35:24.283163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = CNN()\n",
    "print(model)"
   ],
   "id": "fede82aad1686c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): EmptyForward()\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): EmptyForward()\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (8): EmptyForward()\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (11): EmptyForward()\n",
      "    (12): Flatten(start_dim=1, end_dim=-1)\n",
      "    (13): Dropout(p=0, inplace=False)\n",
      "    (14): Linear(in_features=21632, out_features=128, bias=True)\n",
      "    (15): EmptyForward()\n",
      "    (16): ReLU()\n",
      "    (17): Dropout(p=0, inplace=False)\n",
      "    (18): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (19): EmptyForward()\n",
      "    (20): ReLU()\n",
      "    (21): Dropout(p=0, inplace=False)\n",
      "    (22): Linear(in_features=64, out_features=10, bias=True)\n",
      "    (23): EmptyForward()\n",
      "    (24): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T14:35:31.031417Z",
     "start_time": "2024-04-18T14:35:30.507373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary(model, (1, 32, 32))\n",
    "model.to(device)"
   ],
   "id": "61faaea3a48215e9",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x12800 and 21632x128)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43msummary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torchsummary/torchsummary.py:72\u001B[0m, in \u001B[0;36msummary\u001B[0;34m(model, input_size, batch_size, device)\u001B[0m\n\u001B[1;32m     68\u001B[0m model\u001B[38;5;241m.\u001B[39mapply(register_hook)\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# make a forward pass\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;66;03m# print(x.shape)\u001B[39;00m\n\u001B[0;32m---> 72\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# remove these hooks\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m h \u001B[38;5;129;01min\u001B[39;00m hooks:\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[28], line 38\u001B[0m, in \u001B[0;36mCNN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/module.py:1561\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1558\u001B[0m     bw_hook \u001B[38;5;241m=\u001B[39m hooks\u001B[38;5;241m.\u001B[39mBackwardHook(\u001B[38;5;28mself\u001B[39m, full_backward_hooks, backward_pre_hooks)\n\u001B[1;32m   1559\u001B[0m     args \u001B[38;5;241m=\u001B[39m bw_hook\u001B[38;5;241m.\u001B[39msetup_input_hook(args)\n\u001B[0;32m-> 1561\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1562\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks:\n\u001B[1;32m   1563\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook_id, hook \u001B[38;5;129;01min\u001B[39;00m (\n\u001B[1;32m   1564\u001B[0m         \u001B[38;5;241m*\u001B[39m_global_forward_hooks\u001B[38;5;241m.\u001B[39mitems(),\n\u001B[1;32m   1565\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mitems(),\n\u001B[1;32m   1566\u001B[0m     ):\n\u001B[1;32m   1567\u001B[0m         \u001B[38;5;66;03m# mark that always called hook is run\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (2x12800 and 21632x128)"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axs = plt.subplots(1, 10, figsize=(15, 15))\n",
    "for i in range(10):\n",
    "    axs[i].imshow(training_data.data[training_data.targets == i][0], cmap='gray')\n",
    "    axs[i].set_title(training_data.classes[i] + f': {i}')\n",
    "    axs[i].axis('off')"
   ],
   "id": "6f6f80203e3cf3f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_plot_model(model):\n",
    "    n_epochs = 10\n",
    "    n_batch = 1024\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    train_loader = DataLoader(training_data, batch_size=n_batch, shuffle=True)\n",
    "    train_loader_all = DataLoader(training_data, batch_size=len(training_data), shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=True)\n",
    "\n",
    "    print('Moving data to device...')\n",
    "    train_loader = [(data.to(device), target.to(device)) for data, target in train_loader]\n",
    "    train_loader_all = [(data.to(device), target.to(device)) for data, target in train_loader_all]\n",
    "    test_loader = [(data.to(device), target.to(device)) for data, target in test_loader]\n",
    "    print(f'Data moved to device {device}')\n",
    "\n",
    "    train_acc, val_acc = [], []\n",
    "    train_loss, val_loss = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # enumerate mini batches\n",
    "        print(f'Epoch: {epoch + 1}')\n",
    "        for batch_idx, (train_x, train_y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            yhat = model(train_x)\n",
    "            loss = loss_fn(yhat, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            for train_x, train_y in train_loader_all:\n",
    "                yhat = model(train_x)\n",
    "                train_loss.append(loss_fn(yhat, train_y).item())\n",
    "                train_acc.append(((yhat.argmax(1) == train_y).float().mean()).to('cpu'))\n",
    "                print(f'Training   -> Loss: {train_loss[-1]:.3f}, Accuracy: {train_acc[-1]:.3f}')\n",
    "            for test_x, test_y in test_loader:\n",
    "                yhat = model(test_x)\n",
    "                val_loss.append(loss_fn(yhat, test_y).item())\n",
    "                val_acc.append(((yhat.argmax(1) == test_y).float().mean()).to('cpu'))\n",
    "                print(f'Validation -> Loss: {val_loss[-1]:.3f}, Accuracy: {val_acc[-1]:.3f}')\n",
    "        print('------------------------------------------------')\n",
    "    plt.figure(1, figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return {\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc\n",
    "    }"
   ],
   "id": "7faef8464fb04a72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a93b99d6c218f2ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
