{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### G27\n",
    "Dave Brunner"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61d2e7ff1874ab13"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-13T15:16:08.305143Z",
     "start_time": "2024-03-13T15:16:08.302828Z"
    }
   },
   "id": "07ec37d7-f7cb-4646-a2dd-9dab66239a30",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is not available\n",
      "mps would be available, but not set\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"cuda would be available, but not set\")\n",
    "    # torch.set_default_device('cuda')\n",
    "else:\n",
    "    print(\"cuda is not available\")\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"mps would be available, but not set\")\n",
    "    mps_device = torch.device(\"mps\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T15:16:08.332466Z",
     "start_time": "2024-03-13T15:16:08.330096Z"
    }
   },
   "id": "ead81851fdb40f29",
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "id": "e2d8a986-1cb2-483b-9bd8-0f71d69fa9e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4ba6b0c6-3f55-40dd-ba32-6f6aad8f1437",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T15:16:08.379842Z",
     "start_time": "2024-03-13T15:16:08.333329Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data = datasets.mnist.FashionMNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.mnist.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c2da4ed-af9a-4fc7-b59f-135b51241d2f",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-13T15:16:08.386309Z",
     "start_time": "2024-03-13T15:16:08.380580Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data, validation_data = torch.utils.data.random_split(training_data, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "22b30c81-d89f-4947-aa09-ab8c70dd262c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T15:16:08.389352Z",
     "start_time": "2024-03-13T15:16:08.387303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data), len(validation_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6832efcb-c191-42f2-93f5-60ff2e99e934",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 2: Fixed MLP with Increasing Training Dataset\n",
    "\n",
    "Create a MLP with one hidden layer with 200 units for Fashion MNIST classification. Use ReLU activation.\n",
    "\n",
    "Use a random fraction of the training set (split above) to perform the training. Always use the same validation set.\n",
    "\n",
    "Use SGD and cross-entropy loss and suitable learning rate.\n",
    "\n",
    "Start with a single small batch for training (batch size 8) and make sure that you can overfit, i.e. bring the training accuracy to 100%.\n",
    "\n",
    "Then, gradually increase the training set. Let it grow until you obtain values for the training and the validation loss which no longer indicate overfitting. Use a fixed batch size (batchsize 32)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65fabc-6c12-4e88-8a47-0f5ce337103d",
   "metadata": {},
   "source": [
    "#### MLP Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b02cf040-3853-4e48-afa7-4666f8c46d75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T15:20:48.936839Z",
     "start_time": "2024-03-13T15:20:48.930029Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a method that provides an instance of an MLP which uses as a list of units per layer as input\n",
    "# partial_training_data, rest = torch.utils.data.random_split(training_data, [2000, 48000])\n",
    "\n",
    "\n",
    "def mlp(units=[28 * 28, 200, 10]):\n",
    "    class MLP(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MLP, self).__init__()\n",
    "            self.sequential = torch.nn.Sequential(\n",
    "                torch.nn.Flatten(),\n",
    "                torch.nn.Linear(units[0], units[1]),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(units[1], units[2])\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.sequential(x)\n",
    "\n",
    "    return MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5c108c38-1e25-4353-b95a-1ecb00c43fb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T15:20:49.110463Z",
     "start_time": "2024-03-13T15:20:49.104792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 200]         157,000\n",
      "              ReLU-3                  [-1, 200]               0\n",
      "            Linear-4                   [-1, 10]           2,010\n",
      "================================================================\n",
      "Total params: 159,010\n",
      "Trainable params: 159,010\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.61\n",
      "Estimated Total Size (MB): 0.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# create an instance and its summary\n",
    "\n",
    "model = mlp()\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bfd65c-d84d-401e-a209-8d4de656fee3",
   "metadata": {},
   "source": [
    "#### Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c6979eb0-1b61-44df-8f83-3ad42bc6e226",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T15:20:55.834688Z",
     "start_time": "2024-03-13T15:20:55.827257Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_eval(model, lr, nepochs, nbatch, training_data, validation_data):\n",
    "    cost_hist = []\n",
    "    cost_hist_validation = []\n",
    "    acc_hist = []\n",
    "    acc_hist_validation = []\n",
    "\n",
    "    cost_fn = torch.nn.CrossEntropyLoss()\n",
    "    train_loader = DataLoader(training_data, batch_size=nbatch, shuffle=True)\n",
    "    val_loader = DataLoader(validation_data, batch_size=nbatch, shuffle=True)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    iterations_pred_epoch = math.ceil(len(training_data) / nbatch)\n",
    "\n",
    "    for epoch in range(nepochs + 1):\n",
    "        acc_train = 0\n",
    "        loss_train = 0\n",
    "        for batch_idx, (train_x, train_y) in enumerate(train_loader):\n",
    "            pred_train = model(train_x)\n",
    "            loss = cost_fn(pred_train, train_y)\n",
    "\n",
    "            # calculate performance scores\n",
    "            acc_train += (pred_train.argmax(1).eq(train_y)).type(torch.float).sum().item()\n",
    "            loss_train += loss.item()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        cost_hist.append(loss_train / iterations_pred_epoch)\n",
    "        acc_hist.append(acc_train / iterations_pred_epoch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in enumerate(val_loader):\n",
    "                pred = model(test_data)\n",
    "                loss = cost_fn(pred, test_data)\n",
    "                cost_hist_validation.append(loss.item())\n",
    "                acc_val = (pred.argmax(1).eq(val_y)).type(torch.float).sum().item()\n",
    "                acc_hist_validation.append(acc_val)\n",
    "        print(f\"Epoch: {epoch + 1} validation accuracy of {acc_val}\")\n",
    "\n",
    "    return cost_hist, cost_hist_validation, acc_hist, acc_hist_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f7a9da-8c34-42c4-8907-3cc7990227e4",
   "metadata": {},
   "source": [
    "#### First Training\n",
    "\n",
    "Run a first training with only one small training batch (e.g. with a single batch of 64 samples). \n",
    "The small training set can be created by using the functionality `torch.utils.data.random_split` already used above. As validation set use the `validation_data` created above.  \n",
    "\n",
    "This training run can be used to test whether the model and training loop are properly implemented. Explain why and in what sense it can be used as test.\n",
    "\n",
    "This is something you can always do when training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "37778e0b-7f07-4af8-8f9e-45e6ba864248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T15:21:01.172314Z",
     "start_time": "2024-03-13T15:20:59.444955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x16b562420>\n",
      "32 49968\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FashionMNIST' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[92], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(trainset), \u001B[38;5;28mlen\u001B[39m(rest))\n\u001B[1;32m     12\u001B[0m model \u001B[38;5;241m=\u001B[39m mlp([\u001B[38;5;241m28\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m28\u001B[39m, \u001B[38;5;241m200\u001B[39m, \u001B[38;5;241m10\u001B[39m])\n\u001B[0;32m---> 13\u001B[0m cost_train, cost_valid, acc_train, acc_valid \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     16\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(\u001B[38;5;28mrange\u001B[39m(nepochs), cost_train, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb-\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[91], line 34\u001B[0m, in \u001B[0;36mtrain_eval\u001B[0;34m(model, lr, nepochs, nbatch, training_data, validation_data)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m val_x, val_y \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(val_loader):\n\u001B[0;32m---> 34\u001B[0m         pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m         loss \u001B[38;5;241m=\u001B[39m cost_fn(pred, test_data)\n\u001B[1;32m     36\u001B[0m         cost_hist_validation\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[88], line 17\u001B[0m, in \u001B[0;36mmlp.<locals>.MLP.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msequential\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/flatten.py:49\u001B[0m, in \u001B[0;36mFlatten.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten\u001B[49m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mend_dim)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'FashionMNIST' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "nbatch = 32\n",
    "nbatches = 1\n",
    "nepochs = 100\n",
    "lr = 0.1\n",
    "\n",
    "trainsize = nbatches * nbatch\n",
    "trainset, rest = torch.utils.data.random_split(training_data, [trainsize, 50000 - trainsize])\n",
    "\n",
    "print(trainset)\n",
    "print(len(trainset), len(rest))\n",
    "\n",
    "model = mlp([28 * 28, 200, 10])\n",
    "cost_train, cost_valid, acc_train, acc_valid = train_eval(model, lr, nepochs, nbatch, trainset.dataset, validation_data.dataset)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(range(nepochs), cost_train, \"b-\")\n",
    "plt.plot(range(nepochs), cost_valid, \"r-\")\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(range(nepochs), acc_train, \"b-\")\n",
    "plt.plot(range(nepochs), acc_valid, \"r-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c8ceb-8083-4e62-b123-b026aef32e22",
   "metadata": {},
   "source": [
    "#### Evaluate Train and Validation Performance \n",
    "\n",
    "Now run several trainings with the same small model (one hidden layer) and explore for different number of training samples (different number of batches with 32 samples) used, how the train and validation performance evolve (cost and accuracy). Make sure that you train sufficiently long to obtain representative values for cost and accuracy with the given settings. Always use the same validation set (with 10'000 samples).\n",
    "\n",
    "Create plots with training and validation performance vs number of training batches (one for cost and one for accuracy). Use the performance characteristics obtained at the end.\n",
    "\n",
    "Discuss the whether there is a sufficient number of training samples for the given problem at hand. Specify a minimum number if applicable. Also consider whether you are in the underfitting regime.\n",
    "\n",
    "Hint: Specify a list with the number of training batches you would like to perform trainings. Try to be economic with the resources used - try to keep the number of trainings limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0bb95-7392-46a5-9481-640e9177d5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5e0c6ba-8f5e-4470-b110-1e52d86baa59",
   "metadata": {},
   "source": [
    "#### Comments: YOUR comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4d08b0-8ad2-4fc1-98e8-39dd29f204a1",
   "metadata": {},
   "source": [
    "### Exercise 3: Evaluate Different Model Complexities\n",
    "\n",
    "Use the same functionality implemented above (create MLP model, train and evaluate model) to evaluate different model complexities: Number of layers and number of units per layer.\n",
    "\n",
    "Start with the small model used in Exercise 2. Then gradually increase the model complexity. Do this along two dimensions:\n",
    "* a single hidden layer, but increasing the number of units.\n",
    "* a fixed number of units per (hidden) layer, but increase the number of layers.\n",
    "Make sure that you reach the overfitting regime (in either case).\n",
    "\n",
    "Always use the full training set with 50'000 samples.\n",
    "\n",
    "Again make sure that you train sufficiently long so that the obtained train and validation performance measures (cost, accuracy) are representative.\n",
    "\n",
    "Create plots with training and validation performance (cost, accuracy) vs model complexity - one plot with number of units for the single hidden layer case, and one for varying number of layers. \n",
    "\n",
    "Again use the performance characteristics obtained at the end. \n",
    "\n",
    "Finally, discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dcd0fc-3fd6-4b07-a594-e7f6a6336e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #\n",
    "# several iterations with the code snippets of the form:\n",
    "\n",
    "nbatch = 64\n",
    "nepochs =\n",
    "lr =\n",
    "\n",
    "model =\n",
    "\n",
    "cost_train, cost_valid, acc_train, acc_valid = train_eval(model, lr, nepochs, nbatch, training_data, validation_data)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(range(nepochs), cost_train, \"b-\")\n",
    "plt.plot(range(nepochs), cost_valid, \"r-\")\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(range(nepochs), acc_train, \"b-\")\n",
    "plt.plot(range(nepochs), acc_valid, \"r-\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e7893-e293-4ae6-a983-c335ff0c4b21",
   "metadata": {},
   "source": [
    "#### Comments: YOUR findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61beca-5f13-404c-9c9c-568ccca8a00a",
   "metadata": {},
   "source": [
    "### Exercise 4: Add Regularisation\n",
    "\n",
    "Finally, add regularisation - dropout or L1/L2-regularisation. \n",
    "\n",
    "To this end, you need to implement new functionality to instantiate the model.\n",
    "\n",
    "Start with one of the overfitting cases of Exercise 3 and try to improve the validation performance by adding regularisation. You can use either dropout or L1/L2-regularisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b15df-a84b-483d-9365-d8f66390b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a method that provides an instance of an MLP incl regularisation which uses as a list of units per layer as input \n",
    "\n",
    "def mlp_dropout(units=[28 * 28, 200, 10], p_in=0.2, p_hidden=0.5):\n",
    "    \"\"\"\n",
    "    Creates an instance of an MLP with layers as specified in the 'units'-list (list of integers) and dropout \n",
    "    regularisation. Dropout rate for all layers the same except for the first (p_in). For the output layer \n",
    "    no dropout applied. \n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfd0a4-999c-4690-9ef3-b5ee1ab26c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = mlp_dropout([28 * 28, 200, 10])\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa393a-9f99-48f4-996d-cf8d2a3b8cd5",
   "metadata": {},
   "source": [
    "#### Playing with different complexities and regularisation\n",
    "\n",
    "Now play with different complexities and regularisation. \n",
    "Start with one of the overfitting cases identified in the previous exercise.\n",
    "By adding regularisation, you should be able to make it non-overfitting, i.e. generalising better.\n",
    "Note that for a given complexity, adding regularisation reduces the model capacity. This may need to be compensated by increasing the complexity of the model. \n",
    "\n",
    "Use again cost and accuracy for train and validation set to evaluate the results.\n",
    "\n",
    "Finally, estimate the bias error and the generalisation error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa1c2b-50d7-4b23-91c8-c47eec5fc8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0bef3-7aeb-4329-aa64-e8d107c9bee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
