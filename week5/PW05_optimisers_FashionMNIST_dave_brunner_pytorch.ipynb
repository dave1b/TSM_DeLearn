{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "07ec37d7-f7cb-4646-a2dd-9dab66239a30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:18:47.686429Z",
     "start_time": "2024-03-27T14:18:47.683052Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn  # to load all neural net functionality\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8a986-1cb2-483b-9bd8-0f71d69fa9e8",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4ba6b0c6-3f55-40dd-ba32-6f6aad8f1437",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T12:14:19.497783Z",
     "start_time": "2024-03-27T12:14:19.444998Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data = datasets.mnist.FashionMNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.mnist.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c2da4ed-af9a-4fc7-b59f-135b51241d2f",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T12:14:19.507334Z",
     "start_time": "2024-03-27T12:14:19.499066Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data, validation_data = torch.utils.data.random_split(training_data, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "22b30c81-d89f-4947-aa09-ab8c70dd262c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T12:14:19.510392Z",
     "start_time": "2024-03-27T12:14:19.508162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data), len(validation_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ff404-5b8a-4f63-9730-d6708d2ac8d1",
   "metadata": {},
   "source": [
    "### CNN Baseline Model\n",
    "Model with two CNN layers (including max pooling), one dense and an output classification layer, with suitable number of filters and units, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b61b15df-a84b-483d-9365-d8f66390b8bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T15:09:55.297413Z",
     "start_time": "2024-03-27T15:09:55.291256Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "def cnn_model(num_classes=10):\n",
    "    class CNNModel(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super(CNNModel, self).__init__()\n",
    "            # First CNN layer\n",
    "            self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "            self.relu1 = nn.ReLU()\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "            # Second CNN layer\n",
    "            self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "            self.relu2 = nn.ReLU()\n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "            # Dense layer\n",
    "            self.fc1 = nn.Linear(32 * 7 * 7, 128)  # Assuming input image size is 28x28 after pooling\n",
    "            self.relu3 = nn.ReLU()\n",
    "\n",
    "            # Output layer\n",
    "            self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Convolutional layers\n",
    "            x = self.pool1(self.relu1(self.conv1(x)))\n",
    "            x = self.pool2(self.relu2(self.conv2(x)))\n",
    "\n",
    "            # Flatten\n",
    "            x = x.view(-1, 32 * 7 * 7)  # Reshape for the fully connected layer\n",
    "\n",
    "            # Dense layer\n",
    "            x = self.relu3(self.fc1(x))\n",
    "\n",
    "            # Output layer\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    return CNNModel(num_classes)\n",
    "\n",
    "\n",
    "model_description = \"Conv filters=16, kernel=3, s=1, p=1 -> ReLU -> Conv filters=32, kernels=3, s=1, p=1 -> ReLU -> MaxPool k=2, s=2 -> Dense 128 -> ReLU -> Output 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d4bfd0a4-999c-4690-9ef3-b5ee1ab26c30",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:09:55.863375Z",
     "start_time": "2024-03-27T15:09:55.858589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             160\n",
      "              ReLU-2           [-1, 16, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
      "            Conv2d-4           [-1, 32, 14, 14]           4,640\n",
      "              ReLU-5           [-1, 32, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
      "            Linear-7                  [-1, 128]         200,832\n",
      "              ReLU-8                  [-1, 128]               0\n",
      "            Linear-9                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 206,922\n",
      "Trainable params: 206,922\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.33\n",
      "Params size (MB): 0.79\n",
      "Estimated Total Size (MB): 1.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = cnn_model()\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "\n",
    "class Result:\n",
    "    def __init__(self, network, epoch: int, batch_size: int, train_accuracy: [float], val_accuracy: [float],\n",
    "                 train_loss: [float], val_loss: [float], cm: any):\n",
    "        self.network = network,\n",
    "        self.epoch = epoch,\n",
    "        self.batch_size = batch_size\n",
    "        self.train_loss = train_loss\n",
    "        self.val_loss = val_loss\n",
    "        self.train_accuracy = train_accuracy\n",
    "        self.val_accuracy = val_accuracy\n",
    "        self.cm = cm\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Network: {self.network}, Epochs: {self.epoch}, Batch size: {self.batch_size}, Final accuracy: [train:{self.train_accuracy[-1]}, val:{self.val_accuracy[-1]}] Final loss: [train:{self.train_loss[-1]}, val:{self.val_loss[-1]}'\n",
    "\n",
    "    def title(self):\n",
    "        return f'Network: {self.network}, Epochs: {self.epoch}, Batch: {self.batch_size}'\n",
    "\n",
    "    def plot(self, plot_cm):\n",
    "        f = plt.figure(figsize=(12, 4))\n",
    "        ax1 = f.add_subplot(121)\n",
    "        ax2 = f.add_subplot(122)\n",
    "        ax1.plot(self.train_loss, label='Training loss')\n",
    "        ax1.plot(self.val_loss, label='Validation loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid()\n",
    "        ax2.plot(self.train_accuracy, label='Training acc')\n",
    "        ax2.plot(self.val_accuracy, label='Validation acc')\n",
    "        ax2.legend()\n",
    "        ax2.grid()\n",
    "        if plot_cm:\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "            disp.plot(colorbar=False, cmap='Blues')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_result(self, cm=False):\n",
    "        print(self.title())\n",
    "        print(f\"Final val accuracy: {self.val_accuracy[-1]}\")\n",
    "        self.plot(cm)\n",
    "        print(\"--------------------------------------------\")\n",
    "\n",
    "    def print_results(self):\n",
    "        print(\"--------------------------------------------\")\n",
    "        print(self.title())\n",
    "        print(f\"Train accuracy: {self.train_accuracy[-1]}\")\n",
    "        print(f\"Validation accuracy: {self.val_accuracy[-1]}\")\n",
    "\n",
    "\n",
    "results: [Result] = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T12:14:20.400318Z",
     "start_time": "2024-03-27T12:14:19.556952Z"
    }
   },
   "id": "69e6b6fba35c6cae",
   "execution_count": 71
  },
  {
   "cell_type": "markdown",
   "id": "c3fa393a-9f99-48f4-996d-cf8d2a3b8cd5",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Implement the training / evaluation loop\n",
    "Remember training / validation cost and accuracy per epoch and return them as list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "172c91d1-4c9e-413a-bfff-01f51a1e323a",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-27T15:05:00.458455Z",
     "start_time": "2024-03-27T15:04:43.950790Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[106], line 69\u001B[0m\n\u001B[1;32m     66\u001B[0m training_loader \u001B[38;5;241m=\u001B[39m DataLoader(training_data, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     67\u001B[0m validation_loader \u001B[38;5;241m=\u001B[39m DataLoader(validation_data, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 69\u001B[0m cost_hist, cost_hist_test, acc_hist, acc_hist_test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m                                                                \u001B[49m\u001B[43mvalidation_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     71\u001B[0m results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m     72\u001B[0m     Result(model_description, n_epochs, batch_size, acc_hist, acc_hist_test, cost_hist, cost_hist_test, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m     73\u001B[0m results[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mplot_result()\n",
      "Cell \u001B[0;32mIn[106], line 29\u001B[0m, in \u001B[0;36mtrain_eval\u001B[0;34m(model, optimizer, nepochs, training_loader, test_loader, scheduler, verbose)\u001B[0m\n\u001B[1;32m     27\u001B[0m yhat \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mforward(train_x)\n\u001B[1;32m     28\u001B[0m loss \u001B[38;5;241m=\u001B[39m cost_ce(yhat, train_y)\n\u001B[0;32m---> 29\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     30\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     32\u001B[0m acc \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (yhat\u001B[38;5;241m.\u001B[39margmax(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m==\u001B[39m train_y)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mmean()\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_312_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:1103\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:1061\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers-pro/jupyter_debug/pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[0;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[1;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[0;32m--> 169\u001B[0m         \u001B[43mmain_debugger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1184\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1199\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def train_eval(model, optimizer, nepochs, training_loader, test_loader, scheduler=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform training and evaluation loop.\n",
    "    :param model: Model to be trained\n",
    "    :param optimizer: Optimiser to use for training\n",
    "    :param nepochs: Number of epochs\n",
    "    :param training_loader: Loader to provide mini-batches of training samples\n",
    "    :param test_loader: Loader to provide mini-batches of validation samples\n",
    "    :param scheduler: Scheduler used for a learning rate schedule\n",
    "    :return: Lists with training and validation cost and accuracy per epoch.\n",
    "    \"\"\"\n",
    "    cost_hist = []\n",
    "    cost_hist_val = []\n",
    "    acc_hist = []\n",
    "    acc_hist_val = []\n",
    "\n",
    "    cost_ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        acc, cost = 0, 0\n",
    "        acc_val, cost_val = 0, 0\n",
    "        batch_count = 0\n",
    "        batch_count_val = 0\n",
    "\n",
    "        for batch_idx, (train_x, train_y) in enumerate(training_loader):\n",
    "            optimizer.zero_grad()\n",
    "            yhat = model.forward(train_x)\n",
    "            loss = cost_ce(yhat, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc += (yhat.argmax(1) == train_y).float().mean()\n",
    "            cost += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (val_x, val_y) in enumerate(validation_loader):\n",
    "                yhat_val = model(val_x)\n",
    "                loss_val = cost_ce(yhat_val, val_y)\n",
    "                acc_val += (yhat_val.argmax(1) == val_y).float().mean()\n",
    "                cost_val += loss_val.item()\n",
    "                batch_count_val += 1\n",
    "\n",
    "        cost_hist.append(cost / batch_count)\n",
    "        acc_hist.append(acc / batch_count)\n",
    "        cost_hist_val.append(cost_val / batch_count_val)\n",
    "        acc_hist_val.append(acc_val / batch_count_val)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1}\")\n",
    "            print(f\"Train:      Accuracy={acc_hist[-1]:.4f}, Cost: {cost_hist[-1]:.4f}\")\n",
    "            print(f\"Validation: Accuracy={acc_hist_val[-1]:.4f}, Cost: {cost_hist_val[-1]:.4f}\")\n",
    "            print(\"------------------------------------------\")\n",
    "    if not verbose:\n",
    "        print(f\"Final Validation Accuracy={acc_hist_val[-1]:.4f} Cost: {cost_hist_val[-1]:.4f}\")\n",
    "        print(f\"Final Training   Accuracy={acc_hist[-1]:.4f} Cost: {cost_hist[-1]:.4f}\")\n",
    "\n",
    "    return cost_hist, cost_hist_val, acc_hist, acc_hist_val\n",
    "\n",
    "\n",
    "n_epochs = 2\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "training_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "cost_hist, cost_hist_test, acc_hist, acc_hist_test = train_eval(model, optimizer, n_epochs, training_loader,\n",
    "                                                                validation_loader, verbose=True)\n",
    "results.append(\n",
    "    Result(model_description, n_epochs, batch_size, acc_hist, acc_hist_test, cost_hist, cost_hist_test, None))\n",
    "results[-1].plot_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd328c",
   "metadata": {},
   "source": [
    "### Train Baseline CNN Model\n",
    "\n",
    "Follow the \"Steps to Test and Tune a Model\" as presented in the lecture.\n",
    "\n",
    "Train the baseline with SGD without momentum and fixed learning rate. Tune the learning rate by this procedure.\n",
    "\n",
    "Determine a suitable number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train:      Accuracy=0.1013, Cost: nan\n",
      "Validation: Accuracy=0.0948, Cost: nan\n",
      "------------------------------------------\n",
      "Epoch 2\n",
      "Train:      Accuracy=0.1010, Cost: nan\n",
      "Validation: Accuracy=0.0945, Cost: nan\n",
      "------------------------------------------\n",
      "Epoch 3\n",
      "Train:      Accuracy=0.1010, Cost: nan\n",
      "Validation: Accuracy=0.0945, Cost: nan\n",
      "------------------------------------------\n",
      "Epoch 4\n",
      "Train:      Accuracy=0.1010, Cost: nan\n",
      "Validation: Accuracy=0.0945, Cost: nan\n",
      "------------------------------------------\n",
      "Epoch 5\n",
      "Train:      Accuracy=0.1010, Cost: nan\n",
      "Validation: Accuracy=0.0942, Cost: nan\n",
      "------------------------------------------\n",
      "Epoch 6\n",
      "Train:      Accuracy=0.1011, Cost: nan\n",
      "Validation: Accuracy=0.0942, Cost: nan\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[100], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m training_loader \u001B[38;5;241m=\u001B[39m DataLoader(training_data, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      7\u001B[0m validation_loader \u001B[38;5;241m=\u001B[39m DataLoader(validation_data, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 9\u001B[0m cost_hist, cost_hist_test, acc_hist, acc_hist_test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m                                                                \u001B[49m\u001B[43mvalidation_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m     12\u001B[0m     Result(model_description, n_epochs, batch_size, acc_hist, acc_hist_test, cost_hist, cost_hist_test, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m     13\u001B[0m results[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mplot_result()\n",
      "Cell \u001B[0;32mIn[72], line 38\u001B[0m, in \u001B[0;36mtrain_eval\u001B[0;34m(model, optimizer, nepochs, training_loader, test_loader, scheduler, verbose)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (val_x, val_y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(validation_loader):\n\u001B[0;32m---> 38\u001B[0m         yhat_val \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_x\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m         loss_val \u001B[38;5;241m=\u001B[39m cost_ce(yhat_val, val_y)\n\u001B[1;32m     40\u001B[0m         acc_val \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (yhat_val\u001B[38;5;241m.\u001B[39margmax(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m==\u001B[39m val_y)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mmean()\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/nn/modules/module.py:1507\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1504\u001B[0m             tracing_state\u001B[38;5;241m.\u001B[39mpop_scope()\n\u001B[1;32m   1505\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[0;32m-> 1507\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrapped_call_impl\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1508\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1509\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "training_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "cost_hist, cost_hist_test, acc_hist, acc_hist_test = train_eval(model, optimizer, n_epochs, training_loader,\n",
    "                                                                validation_loader, verbose=True)\n",
    "results.append(\n",
    "    Result(model_description, n_epochs, batch_size, acc_hist, acc_hist_test, cost_hist, cost_hist_test, None))\n",
    "results[-1].plot_result()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:58:07.838555Z",
     "start_time": "2024-03-27T14:56:40.871706Z"
    }
   },
   "id": "98f5859b87abf5bd",
   "execution_count": 100
  },
  {
   "cell_type": "markdown",
   "id": "532d2c36",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "Use 5-fold cross validation to estimate the accuracy and an error bar of the accuracy estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def cross_validate(network, k_folds, batch_size, loss_function, dataset, num_epochs=1):\n",
    "    # Initialize the k-fold cross validation\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=4)\n",
    "    cross_validation_results = {}\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "\n",
    "        # Print\n",
    "        print('--------------------------------')\n",
    "        print(f'FOLD {fold + 1}')\n",
    "\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size, sampler=train_subsampler)\n",
    "        testloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size, sampler=test_subsampler)\n",
    "\n",
    "        # Initialize optimizer\n",
    "        optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "\n",
    "        # Run the training loop for defined number of epochs\n",
    "        for epoch in range(0, num_epochs):\n",
    "            # Set current loss value\n",
    "            current_loss = 0.0\n",
    "\n",
    "            # Iterate over the DataLoader for training data\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                # Get inputs\n",
    "                inputs, targets = data\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Perform forward pass\n",
    "                outputs = network(inputs)\n",
    "                # Compute loss\n",
    "                loss = loss_function(outputs, targets)\n",
    "                # Perform backward pass\n",
    "                loss.backward()\n",
    "                # Perform optimization\n",
    "                optimizer.step()\n",
    "                # Print statistics\n",
    "                current_loss += loss.item()\n",
    "                if i % 500 == 499:\n",
    "                    print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\n",
    "                    current_loss = 0.0\n",
    "        # Evaluation for this fold\n",
    "        print('Starting testing')\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the test data and generate predictions\n",
    "            for i, data in enumerate(testloader, 0):\n",
    "                # Get inputs\n",
    "                inputs, targets = data\n",
    "                # Generate outputs\n",
    "                outputs = network(inputs)\n",
    "                # Set total and correct\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "            # Print accuracy\n",
    "            print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
    "            cross_validation_results[fold] = 100.0 * (correct / total)\n",
    "\n",
    "    # Print fold results\n",
    "    print('-------------------------------------------------')\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    sum = 0.0\n",
    "    for key, value in cross_validation_results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average over all folds: {sum / len(cross_validation_results.items())} %')\n",
    "    print('-------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:48:53.975874Z",
     "start_time": "2024-03-27T14:48:53.961323Z"
    }
   },
   "id": "aa20d275",
   "execution_count": 98
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "k_folds = 5\n",
    "batch_size = 64\n",
    "num_epochs = 1\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "dataset = ConcatDataset([training_data, validation_data])\n",
    "network = cnn_model()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22ad0ea739b8d2cf"
  },
  {
   "cell_type": "markdown",
   "id": "4625a52b-a332-4844-8d7d-6998893a2d70",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analyse Different Optimisers with different Settings \n",
    "\n",
    "Use the code above to explore different settings for the different optimizers. Use batchsize 64.\n",
    "\n",
    "1. *SGD*: Refer to the results from above - for later comparison.\n",
    "<br>\n",
    "\n",
    "2. *Momentum*: Play with at least three different settings when using momentum: learning rate, momentum parameter, Nesterov flag. Start with momentum=0.9 without Nesterov and suitable learning rate, then vary the momentum parameter and independently the learning rate. Can you see an impact of using Nesterov? What is your recommended best choice (lr, momentum, nesterov, nepochs) for the given problem?\n",
    "<br>\n",
    "\n",
    "3. *RMSProp*: Same thing now for RMSprop (without momentum). Play with at least three different settings when using RMSprop: lr, alpha. Start with the default settings of pytorch with (lr=0.01, alpha=0.99,centered=False). Then vary alpha and independently the learning rate. Can you see an impact when using centered=True? What is your recommended best choice (learning rate, alpha, centered, nepochs) for the given problem?<br>\n",
    "<br>\n",
    "\n",
    "4. *Adam*: Same thing now for Adam. Play with at least three different settings. Start with the default settings of pytorch. What is your recommended best choice for the given problem?<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa1c2b-50d7-4b23-91c8-c47eec5fc8fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T12:15:20.375047Z",
     "start_time": "2024-03-27T12:15:20.374971Z"
    }
   },
   "outputs": [],
   "source": [
    "nbatch = 64\n",
    "nepochs =\n",
    "\n",
    "training_loader = DataLoader(training_data, batch_size=nbatch, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=nbatch, shuffle=True)\n",
    "\n",
    "model = mlp()\n",
    "optimizer = ...\n",
    "cost_hist, cost_hist_test, acc_hist, acc_hist_test = train_eval(model, optimizer, nepochs, training_loader,\n",
    "                                                                validation_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f24c35-9def-48ba-a505-20b99d450584",
   "metadata": {},
   "source": [
    "### Plots and Comments (for the different steps described above) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ab6cb-7787-4f18-8763-ae231c610c68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T12:15:20.375869Z",
     "start_time": "2024-03-27T12:15:20.375813Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35fb4db4",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule\n",
    "\n",
    "Modify your `train_eval` method implemented above to support using a learning rate schedule for SGD (without momentum) - by using e.g. StepLR. What are your preferred settings for the given task?\n",
    "\n",
    "Compare and evaluate the training performance with the results obtained for the different optimizers above and provide a judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a02fc",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-03-27T12:15:20.377247Z",
     "start_time": "2024-03-27T12:15:20.377076Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d237b4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T12:15:20.377833Z",
     "start_time": "2024-03-27T12:15:20.377715Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
