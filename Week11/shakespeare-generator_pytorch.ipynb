{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:19:00.215730Z",
     "start_time": "2024-05-10T09:19:00.213647Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchinfo import summary # with torchsummary we got a bug, consider installing torchinfo\n",
    "from torch.utils.data import DataLoader, TensorDataset # lets us load data in batches\n",
    "from tqdm.notebook import tqdm # with tqdm we got a bug"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:19:00.262729Z",
     "start_time": "2024-05-10T09:19:00.259968Z"
    }
   },
   "source": [
    "# wget the file from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
    "with open(\"shakespeare.txt\") as corpus_file:\n",
    "    corpus = corpus_file.read()\n",
    "    corpus_length = len(corpus)\n",
    "    \n",
    "print(\"Loaded a corpus of {0} characters\".format(corpus_length))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a corpus of 1115394 characters\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:19:00.293253Z",
     "start_time": "2024-05-10T09:19:00.285269Z"
    }
   },
   "source": [
    "# Get a unique identifier for each char in the corpus, \n",
    "# then make some dicts to ease encoding and decoding\n",
    "chars = sorted(list(set(corpus)))\n",
    "num_chars = len(chars)\n",
    "encoding = {c: i for i, c in enumerate(chars)}\n",
    "decoding = {i: c for i, c in enumerate(chars)}\n",
    "print(\"Our corpus contains {0} unique characters.\".format(num_chars))\n",
    "print(corpus[:100])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our corpus contains 65 unique characters.\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:19:00.296291Z",
     "start_time": "2024-05-10T09:19:00.294309Z"
    }
   },
   "source": [
    "print(decoding)\n",
    "print(encoding)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to One  approach"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:19:01.673622Z",
     "start_time": "2024-05-10T09:19:00.300771Z"
    }
   },
   "source": [
    "# chop up our data into X and y, slice into roughly \n",
    "# (num_chars / skip) overlapping 'sentences' of length \n",
    "# sentence_length, and encode the chars\n",
    "sentence_length = 20\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append(encoding[next_char])\n",
    "\n",
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\"\n",
    "      .format(num_sentences, sentence_length))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced our corpus into 1115374 sentences of length 20\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:19:01.676055Z",
     "start_time": "2024-05-10T09:19:01.674430Z"
    }
   },
   "source": [
    "print(X_data[0])\n",
    "print([decoding[idx] for idx in X_data[0]])\n",
    "print(decoding[y_data[0]])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56]\n",
      "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r']\n",
      "e\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:19:07.920484Z",
     "start_time": "2024-05-10T09:19:01.676575Z"
    }
   },
   "source": [
    "# One-hot encode the data.\n",
    "X = F.one_hot(torch.tensor(X_data), num_classes=num_chars).to(torch.float)\n",
    "y = torch.tensor(y_data) #No need to encode labels in one-hot with pytorch,\n",
    "                         #crossEntropy loss needs just the indexes (not 0-1 values)\n",
    "\n",
    "\n",
    "# Double check our vectorized data before we sink hours into fitting a model\n",
    "print(\"Sanity check y. Dimension: {0} # Sentences: {1} Characters in corpus: {2}\"\n",
    "      .format(y.shape, num_sentences, len(chars)))\n",
    "print(\"Sanity check X. Dimension: {0} Sentence length: {1}\"\n",
    "      .format(X.size(), sentence_length))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check y. Dimension: torch.Size([1115374]) # Sentences: 1115374 Characters in corpus: 65\n",
      "Sanity check X. Dimension: torch.Size([1115374, 20, 65]) Sentence length: 20\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:19:07.926318Z",
     "start_time": "2024-05-10T09:19:07.922918Z"
    }
   },
   "source": [
    "# Let's define our model\n",
    "        \n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layer_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Defining the number of h layers and the nodes in each layer\n",
    "        self.layer_size = layer_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, layer_size, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.rnn(x)\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1]\n",
    "        # Convert the final state to our desired output shape (batch_size, output_size)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:19:08.292014Z",
     "start_time": "2024-05-10T09:19:07.926871Z"
    }
   },
   "source": [
    "hidden_size = 256\n",
    "seq_length = 20\n",
    "num_classes = 65\n",
    "layer_size = 1\n",
    "batch_size= 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNNModel(num_classes, hidden_size, layer_size, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "summary(model, input_size=(batch_size, seq_length, num_classes))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "RNNModel                                 [128, 65]                 --\n",
       "├─RNN: 1-1                               [128, 20, 256]            82,688\n",
       "├─Linear: 1-2                            [128, 65]                 16,705\n",
       "==========================================================================================\n",
       "Total params: 99,393\n",
       "Trainable params: 99,393\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 213.82\n",
       "==========================================================================================\n",
       "Input size (MB): 0.67\n",
       "Forward/backward pass size (MB): 5.31\n",
       "Params size (MB): 0.40\n",
       "Estimated Total Size (MB): 6.37\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:19:08.295126Z",
     "start_time": "2024-05-10T09:19:08.292739Z"
    }
   },
   "source": [
    "model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (rnn): RNN(65, 256, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:19:08.297791Z",
     "start_time": "2024-05-10T09:19:08.295936Z"
    }
   },
   "source": [
    "# Create dataloader from X,y tensors\n",
    "dataset = TensorDataset(X, y)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:24:01.706090Z",
     "start_time": "2024-05-10T09:19:08.298487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(epoch, model, data_loader, log_interval=200):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_correct = 0 \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "        # Backpropagate. Updates the gradients buffer on each parameter\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        _, pred = torch.max(output, dim=1)\n",
    "\n",
    "        total_correct += torch.sum(pred == target).item()\n",
    "                  \n",
    "        \n",
    "        # if batch_idx % log_interval == 0:\n",
    "        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #         epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "        #         100. * batch_idx / len(data_loader), loss.data.item()))\n",
    "    \n",
    "    accuracy_train = total_correct / len(data_loader.dataset)\n",
    "\n",
    "    \n",
    "    total_train_loss = total_train_loss / len(data_loader)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_train_loss,\n",
    "        \"accuracy\": accuracy_train,\n",
    "    }\n",
    "# train the model\n",
    "##%%time\n",
    "\n",
    "# Keep track of stats to plot them\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_result = train(epoch, model, train_loader)\n",
    "    train_losses.append(train_result[\"loss\"])\n",
    "    train_accuracies.append(train_result[\"accuracy\"])\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1115374 (0%)]\tLoss: 4.184679\n",
      "Train Epoch: 1 [25600/1115374 (2%)]\tLoss: 2.742207\n",
      "Train Epoch: 1 [51200/1115374 (5%)]\tLoss: 2.481631\n",
      "Train Epoch: 1 [76800/1115374 (7%)]\tLoss: 2.232582\n",
      "Train Epoch: 1 [102400/1115374 (9%)]\tLoss: 2.401128\n",
      "Train Epoch: 1 [128000/1115374 (11%)]\tLoss: 2.344580\n",
      "Train Epoch: 1 [153600/1115374 (14%)]\tLoss: 2.089907\n",
      "Train Epoch: 1 [179200/1115374 (16%)]\tLoss: 2.253219\n",
      "Train Epoch: 1 [204800/1115374 (18%)]\tLoss: 2.185755\n",
      "Train Epoch: 1 [230400/1115374 (21%)]\tLoss: 2.127522\n",
      "Train Epoch: 1 [256000/1115374 (23%)]\tLoss: 2.094757\n",
      "Train Epoch: 1 [281600/1115374 (25%)]\tLoss: 2.091164\n",
      "Train Epoch: 1 [307200/1115374 (28%)]\tLoss: 2.113344\n",
      "Train Epoch: 1 [332800/1115374 (30%)]\tLoss: 2.094473\n",
      "Train Epoch: 1 [358400/1115374 (32%)]\tLoss: 2.146134\n",
      "Train Epoch: 1 [384000/1115374 (34%)]\tLoss: 1.989364\n",
      "Train Epoch: 1 [409600/1115374 (37%)]\tLoss: 1.769158\n",
      "Train Epoch: 1 [435200/1115374 (39%)]\tLoss: 2.125119\n",
      "Train Epoch: 1 [460800/1115374 (41%)]\tLoss: 1.849709\n",
      "Train Epoch: 1 [486400/1115374 (44%)]\tLoss: 2.171310\n",
      "Train Epoch: 1 [512000/1115374 (46%)]\tLoss: 2.033393\n",
      "Train Epoch: 1 [537600/1115374 (48%)]\tLoss: 1.875858\n",
      "Train Epoch: 1 [563200/1115374 (50%)]\tLoss: 1.760847\n",
      "Train Epoch: 1 [588800/1115374 (53%)]\tLoss: 2.128595\n",
      "Train Epoch: 1 [614400/1115374 (55%)]\tLoss: 1.850206\n",
      "Train Epoch: 1 [640000/1115374 (57%)]\tLoss: 1.800829\n",
      "Train Epoch: 1 [665600/1115374 (60%)]\tLoss: 2.034985\n",
      "Train Epoch: 1 [691200/1115374 (62%)]\tLoss: 1.923882\n",
      "Train Epoch: 1 [716800/1115374 (64%)]\tLoss: 1.695313\n",
      "Train Epoch: 1 [742400/1115374 (67%)]\tLoss: 1.935630\n",
      "Train Epoch: 1 [768000/1115374 (69%)]\tLoss: 1.874088\n",
      "Train Epoch: 1 [793600/1115374 (71%)]\tLoss: 1.979725\n",
      "Train Epoch: 1 [819200/1115374 (73%)]\tLoss: 1.747704\n",
      "Train Epoch: 1 [844800/1115374 (76%)]\tLoss: 1.960783\n",
      "Train Epoch: 1 [870400/1115374 (78%)]\tLoss: 1.713532\n",
      "Train Epoch: 1 [896000/1115374 (80%)]\tLoss: 1.796684\n",
      "Train Epoch: 1 [921600/1115374 (83%)]\tLoss: 1.697467\n",
      "Train Epoch: 1 [947200/1115374 (85%)]\tLoss: 1.631904\n",
      "Train Epoch: 1 [972800/1115374 (87%)]\tLoss: 1.866615\n",
      "Train Epoch: 1 [998400/1115374 (90%)]\tLoss: 1.862060\n",
      "Train Epoch: 1 [1024000/1115374 (92%)]\tLoss: 1.830278\n",
      "Train Epoch: 1 [1049600/1115374 (94%)]\tLoss: 1.800827\n",
      "Train Epoch: 1 [1075200/1115374 (96%)]\tLoss: 1.769543\n",
      "Train Epoch: 1 [1100800/1115374 (99%)]\tLoss: 1.803954\n",
      "Train Epoch: 2 [0/1115374 (0%)]\tLoss: 1.859737\n",
      "Train Epoch: 2 [25600/1115374 (2%)]\tLoss: 1.968192\n",
      "Train Epoch: 2 [51200/1115374 (5%)]\tLoss: 1.904243\n",
      "Train Epoch: 2 [76800/1115374 (7%)]\tLoss: 1.806494\n",
      "Train Epoch: 2 [102400/1115374 (9%)]\tLoss: 1.740310\n",
      "Train Epoch: 2 [128000/1115374 (11%)]\tLoss: 1.704280\n",
      "Train Epoch: 2 [153600/1115374 (14%)]\tLoss: 1.826536\n",
      "Train Epoch: 2 [179200/1115374 (16%)]\tLoss: 1.685446\n",
      "Train Epoch: 2 [204800/1115374 (18%)]\tLoss: 1.913452\n",
      "Train Epoch: 2 [230400/1115374 (21%)]\tLoss: 1.986184\n",
      "Train Epoch: 2 [256000/1115374 (23%)]\tLoss: 1.686416\n",
      "Train Epoch: 2 [281600/1115374 (25%)]\tLoss: 1.849083\n",
      "Train Epoch: 2 [307200/1115374 (28%)]\tLoss: 1.704327\n",
      "Train Epoch: 2 [332800/1115374 (30%)]\tLoss: 1.930094\n",
      "Train Epoch: 2 [358400/1115374 (32%)]\tLoss: 1.661689\n",
      "Train Epoch: 2 [384000/1115374 (34%)]\tLoss: 1.626705\n",
      "Train Epoch: 2 [409600/1115374 (37%)]\tLoss: 1.686000\n",
      "Train Epoch: 2 [435200/1115374 (39%)]\tLoss: 1.419091\n",
      "Train Epoch: 2 [460800/1115374 (41%)]\tLoss: 1.822812\n",
      "Train Epoch: 2 [486400/1115374 (44%)]\tLoss: 1.583392\n",
      "Train Epoch: 2 [512000/1115374 (46%)]\tLoss: 1.809483\n",
      "Train Epoch: 2 [537600/1115374 (48%)]\tLoss: 1.801918\n",
      "Train Epoch: 2 [563200/1115374 (50%)]\tLoss: 1.573598\n",
      "Train Epoch: 2 [588800/1115374 (53%)]\tLoss: 1.925788\n",
      "Train Epoch: 2 [614400/1115374 (55%)]\tLoss: 1.739127\n",
      "Train Epoch: 2 [640000/1115374 (57%)]\tLoss: 1.750449\n",
      "Train Epoch: 2 [665600/1115374 (60%)]\tLoss: 1.597527\n",
      "Train Epoch: 2 [691200/1115374 (62%)]\tLoss: 2.023665\n",
      "Train Epoch: 2 [716800/1115374 (64%)]\tLoss: 1.704489\n",
      "Train Epoch: 2 [742400/1115374 (67%)]\tLoss: 1.671005\n",
      "Train Epoch: 2 [768000/1115374 (69%)]\tLoss: 1.902407\n",
      "Train Epoch: 2 [793600/1115374 (71%)]\tLoss: 1.742828\n",
      "Train Epoch: 2 [819200/1115374 (73%)]\tLoss: 1.630682\n",
      "Train Epoch: 2 [844800/1115374 (76%)]\tLoss: 1.847528\n",
      "Train Epoch: 2 [870400/1115374 (78%)]\tLoss: 1.686568\n",
      "Train Epoch: 2 [896000/1115374 (80%)]\tLoss: 1.435918\n",
      "Train Epoch: 2 [921600/1115374 (83%)]\tLoss: 1.792893\n",
      "Train Epoch: 2 [947200/1115374 (85%)]\tLoss: 1.768899\n",
      "Train Epoch: 2 [972800/1115374 (87%)]\tLoss: 1.459521\n",
      "Train Epoch: 2 [998400/1115374 (90%)]\tLoss: 1.801757\n",
      "Train Epoch: 2 [1024000/1115374 (92%)]\tLoss: 1.776061\n",
      "Train Epoch: 2 [1049600/1115374 (94%)]\tLoss: 1.507561\n",
      "Train Epoch: 2 [1075200/1115374 (96%)]\tLoss: 1.783965\n",
      "Train Epoch: 2 [1100800/1115374 (99%)]\tLoss: 1.546228\n",
      "Train Epoch: 3 [0/1115374 (0%)]\tLoss: 1.459582\n",
      "Train Epoch: 3 [25600/1115374 (2%)]\tLoss: 1.585600\n",
      "Train Epoch: 3 [51200/1115374 (5%)]\tLoss: 1.987001\n",
      "Train Epoch: 3 [76800/1115374 (7%)]\tLoss: 1.494500\n",
      "Train Epoch: 3 [102400/1115374 (9%)]\tLoss: 1.860960\n",
      "Train Epoch: 3 [128000/1115374 (11%)]\tLoss: 1.465360\n",
      "Train Epoch: 3 [153600/1115374 (14%)]\tLoss: 1.639107\n",
      "Train Epoch: 3 [179200/1115374 (16%)]\tLoss: 1.645508\n",
      "Train Epoch: 3 [204800/1115374 (18%)]\tLoss: 1.730893\n",
      "Train Epoch: 3 [230400/1115374 (21%)]\tLoss: 1.957116\n",
      "Train Epoch: 3 [256000/1115374 (23%)]\tLoss: 1.522915\n",
      "Train Epoch: 3 [281600/1115374 (25%)]\tLoss: 1.608029\n",
      "Train Epoch: 3 [307200/1115374 (28%)]\tLoss: 1.438636\n",
      "Train Epoch: 3 [332800/1115374 (30%)]\tLoss: 1.384508\n",
      "Train Epoch: 3 [358400/1115374 (32%)]\tLoss: 1.633078\n",
      "Train Epoch: 3 [384000/1115374 (34%)]\tLoss: 1.881554\n",
      "Train Epoch: 3 [409600/1115374 (37%)]\tLoss: 1.759386\n",
      "Train Epoch: 3 [435200/1115374 (39%)]\tLoss: 1.492774\n",
      "Train Epoch: 3 [460800/1115374 (41%)]\tLoss: 1.652248\n",
      "Train Epoch: 3 [486400/1115374 (44%)]\tLoss: 1.491702\n",
      "Train Epoch: 3 [512000/1115374 (46%)]\tLoss: 1.678499\n",
      "Train Epoch: 3 [537600/1115374 (48%)]\tLoss: 1.686789\n",
      "Train Epoch: 3 [563200/1115374 (50%)]\tLoss: 1.554943\n",
      "Train Epoch: 3 [588800/1115374 (53%)]\tLoss: 1.646450\n",
      "Train Epoch: 3 [614400/1115374 (55%)]\tLoss: 1.488394\n",
      "Train Epoch: 3 [640000/1115374 (57%)]\tLoss: 1.777992\n",
      "Train Epoch: 3 [665600/1115374 (60%)]\tLoss: 1.718764\n",
      "Train Epoch: 3 [691200/1115374 (62%)]\tLoss: 1.479687\n",
      "Train Epoch: 3 [716800/1115374 (64%)]\tLoss: 1.616995\n",
      "Train Epoch: 3 [742400/1115374 (67%)]\tLoss: 1.487813\n",
      "Train Epoch: 3 [768000/1115374 (69%)]\tLoss: 1.804107\n",
      "Train Epoch: 3 [793600/1115374 (71%)]\tLoss: 1.804227\n",
      "Train Epoch: 3 [819200/1115374 (73%)]\tLoss: 1.317843\n",
      "Train Epoch: 3 [844800/1115374 (76%)]\tLoss: 1.910178\n",
      "Train Epoch: 3 [870400/1115374 (78%)]\tLoss: 1.605034\n",
      "Train Epoch: 3 [896000/1115374 (80%)]\tLoss: 1.733691\n",
      "Train Epoch: 3 [921600/1115374 (83%)]\tLoss: 1.734982\n",
      "Train Epoch: 3 [947200/1115374 (85%)]\tLoss: 1.646177\n",
      "Train Epoch: 3 [972800/1115374 (87%)]\tLoss: 1.424350\n",
      "Train Epoch: 3 [998400/1115374 (90%)]\tLoss: 1.530053\n",
      "Train Epoch: 3 [1024000/1115374 (92%)]\tLoss: 1.654566\n",
      "Train Epoch: 3 [1049600/1115374 (94%)]\tLoss: 1.685356\n",
      "Train Epoch: 3 [1075200/1115374 (96%)]\tLoss: 1.442263\n",
      "Train Epoch: 3 [1100800/1115374 (99%)]\tLoss: 1.817466\n",
      "Train Epoch: 4 [0/1115374 (0%)]\tLoss: 1.745952\n",
      "Train Epoch: 4 [25600/1115374 (2%)]\tLoss: 1.558254\n",
      "Train Epoch: 4 [51200/1115374 (5%)]\tLoss: 1.211888\n",
      "Train Epoch: 4 [76800/1115374 (7%)]\tLoss: 1.493390\n",
      "Train Epoch: 4 [102400/1115374 (9%)]\tLoss: 1.394314\n",
      "Train Epoch: 4 [128000/1115374 (11%)]\tLoss: 1.519536\n",
      "Train Epoch: 4 [153600/1115374 (14%)]\tLoss: 1.462137\n",
      "Train Epoch: 4 [179200/1115374 (16%)]\tLoss: 1.749902\n",
      "Train Epoch: 4 [204800/1115374 (18%)]\tLoss: 1.619678\n",
      "Train Epoch: 4 [230400/1115374 (21%)]\tLoss: 1.840706\n",
      "Train Epoch: 4 [256000/1115374 (23%)]\tLoss: 1.365404\n",
      "Train Epoch: 4 [281600/1115374 (25%)]\tLoss: 1.731360\n",
      "Train Epoch: 4 [307200/1115374 (28%)]\tLoss: 1.650368\n",
      "Train Epoch: 4 [332800/1115374 (30%)]\tLoss: 1.667102\n",
      "Train Epoch: 4 [358400/1115374 (32%)]\tLoss: 1.685090\n",
      "Train Epoch: 4 [384000/1115374 (34%)]\tLoss: 1.413781\n",
      "Train Epoch: 4 [409600/1115374 (37%)]\tLoss: 1.603420\n",
      "Train Epoch: 4 [435200/1115374 (39%)]\tLoss: 1.711937\n",
      "Train Epoch: 4 [460800/1115374 (41%)]\tLoss: 1.761482\n",
      "Train Epoch: 4 [486400/1115374 (44%)]\tLoss: 1.652273\n",
      "Train Epoch: 4 [512000/1115374 (46%)]\tLoss: 1.606288\n",
      "Train Epoch: 4 [537600/1115374 (48%)]\tLoss: 1.595853\n",
      "Train Epoch: 4 [563200/1115374 (50%)]\tLoss: 1.696886\n",
      "Train Epoch: 4 [588800/1115374 (53%)]\tLoss: 1.590708\n",
      "Train Epoch: 4 [614400/1115374 (55%)]\tLoss: 1.774775\n",
      "Train Epoch: 4 [640000/1115374 (57%)]\tLoss: 1.402567\n",
      "Train Epoch: 4 [665600/1115374 (60%)]\tLoss: 1.762226\n",
      "Train Epoch: 4 [691200/1115374 (62%)]\tLoss: 1.420730\n",
      "Train Epoch: 4 [716800/1115374 (64%)]\tLoss: 1.621402\n",
      "Train Epoch: 4 [742400/1115374 (67%)]\tLoss: 1.697835\n",
      "Train Epoch: 4 [768000/1115374 (69%)]\tLoss: 1.501018\n",
      "Train Epoch: 4 [793600/1115374 (71%)]\tLoss: 1.389930\n",
      "Train Epoch: 4 [819200/1115374 (73%)]\tLoss: 1.607355\n",
      "Train Epoch: 4 [844800/1115374 (76%)]\tLoss: 1.400771\n",
      "Train Epoch: 4 [870400/1115374 (78%)]\tLoss: 2.020625\n",
      "Train Epoch: 4 [896000/1115374 (80%)]\tLoss: 1.413103\n",
      "Train Epoch: 4 [921600/1115374 (83%)]\tLoss: 1.568735\n",
      "Train Epoch: 4 [947200/1115374 (85%)]\tLoss: 1.775524\n",
      "Train Epoch: 4 [972800/1115374 (87%)]\tLoss: 1.803499\n",
      "Train Epoch: 4 [998400/1115374 (90%)]\tLoss: 1.629416\n",
      "Train Epoch: 4 [1024000/1115374 (92%)]\tLoss: 1.505598\n",
      "Train Epoch: 4 [1049600/1115374 (94%)]\tLoss: 1.396051\n",
      "Train Epoch: 4 [1075200/1115374 (96%)]\tLoss: 1.510180\n",
      "Train Epoch: 4 [1100800/1115374 (99%)]\tLoss: 1.610745\n",
      "Train Epoch: 5 [0/1115374 (0%)]\tLoss: 1.303915\n",
      "Train Epoch: 5 [25600/1115374 (2%)]\tLoss: 1.491576\n",
      "Train Epoch: 5 [51200/1115374 (5%)]\tLoss: 1.615950\n",
      "Train Epoch: 5 [76800/1115374 (7%)]\tLoss: 1.774952\n",
      "Train Epoch: 5 [102400/1115374 (9%)]\tLoss: 1.521281\n",
      "Train Epoch: 5 [128000/1115374 (11%)]\tLoss: 1.267933\n",
      "Train Epoch: 5 [153600/1115374 (14%)]\tLoss: 1.430438\n",
      "Train Epoch: 5 [179200/1115374 (16%)]\tLoss: 1.558139\n",
      "Train Epoch: 5 [204800/1115374 (18%)]\tLoss: 1.505273\n",
      "Train Epoch: 5 [230400/1115374 (21%)]\tLoss: 1.591370\n",
      "Train Epoch: 5 [256000/1115374 (23%)]\tLoss: 1.721339\n",
      "Train Epoch: 5 [281600/1115374 (25%)]\tLoss: 1.677746\n",
      "Train Epoch: 5 [307200/1115374 (28%)]\tLoss: 1.787437\n",
      "Train Epoch: 5 [332800/1115374 (30%)]\tLoss: 1.386163\n",
      "Train Epoch: 5 [358400/1115374 (32%)]\tLoss: 1.592359\n",
      "Train Epoch: 5 [384000/1115374 (34%)]\tLoss: 1.622792\n",
      "Train Epoch: 5 [409600/1115374 (37%)]\tLoss: 1.586690\n",
      "Train Epoch: 5 [435200/1115374 (39%)]\tLoss: 1.512687\n",
      "Train Epoch: 5 [460800/1115374 (41%)]\tLoss: 1.748003\n",
      "Train Epoch: 5 [486400/1115374 (44%)]\tLoss: 1.616313\n",
      "Train Epoch: 5 [512000/1115374 (46%)]\tLoss: 1.545550\n",
      "Train Epoch: 5 [537600/1115374 (48%)]\tLoss: 1.710119\n",
      "Train Epoch: 5 [563200/1115374 (50%)]\tLoss: 1.638803\n",
      "Train Epoch: 5 [588800/1115374 (53%)]\tLoss: 1.416237\n",
      "Train Epoch: 5 [614400/1115374 (55%)]\tLoss: 1.985265\n",
      "Train Epoch: 5 [640000/1115374 (57%)]\tLoss: 1.779968\n",
      "Train Epoch: 5 [665600/1115374 (60%)]\tLoss: 1.731488\n",
      "Train Epoch: 5 [691200/1115374 (62%)]\tLoss: 1.444778\n",
      "Train Epoch: 5 [716800/1115374 (64%)]\tLoss: 1.535235\n",
      "Train Epoch: 5 [742400/1115374 (67%)]\tLoss: 1.585426\n",
      "Train Epoch: 5 [768000/1115374 (69%)]\tLoss: 1.589442\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 52\u001B[0m\n\u001B[1;32m     50\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m---> 52\u001B[0m     train_result \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(train_result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     54\u001B[0m     train_accuracies\u001B[38;5;241m.\u001B[39mappend(train_result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "Cell \u001B[0;32mIn[16], line 18\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(epoch, model, data_loader, log_interval)\u001B[0m\n\u001B[1;32m     16\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Update weights\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m total_train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     22\u001B[0m _, pred \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(output, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/optim/optimizer.py:391\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    386\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    387\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    388\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    389\u001B[0m             )\n\u001B[0;32m--> 391\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    394\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/optim/adam.py:168\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    157\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    159\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    160\u001B[0m         group,\n\u001B[1;32m    161\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    165\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    166\u001B[0m         state_steps)\n\u001B[0;32m--> 168\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/optim/adam.py:318\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    316\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 318\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    320\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    321\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    322\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    323\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    324\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[43m     \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    326\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    327\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    328\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    330\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    331\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    332\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    334\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/optim/adam.py:443\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    441\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (exp_avg_sq\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[0;32m--> 443\u001B[0m     \u001B[43mparam\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddcdiv_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexp_avg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdenom\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mstep_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    445\u001B[0m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n\u001B[1;32m    446\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m amsgrad \u001B[38;5;129;01mand\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mis_complex(params[i]):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:24:01.708033Z",
     "start_time": "2024-05-10T09:24:01.707787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.inference_mode() \n",
    "def predict(model, data):\n",
    "    # Put the model in eval mode, which disables training specific behaviour.\n",
    "    model.eval()\n",
    "    output = model(data)\n",
    "    return output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.legend()\n",
    "plt.grid()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def make_seed(seed_phrase=\"\"):\n",
    "        if seed_phrase:  # make sure the seed has the right length\n",
    "            phrase_length = len(seed_phrase)\n",
    "            pattern = \"\"\n",
    "            for i in range (0, sentence_length):\n",
    "                pattern += seed_phrase[i % phrase_length]\n",
    "        else:            # sample randomly the seed from corpus\n",
    "            seed = random.randint(0, corpus_length - sentence_length)\n",
    "            pattern = corpus[seed:seed + sentence_length]\n",
    "        return pattern\n",
    "    \n",
    "    \n",
    "\n",
    "seed_pattern = make_seed(\"Once upon a time in \")\n",
    "print(\"seed = \" + seed_pattern)\n",
    "\n",
    "encoded_text = torch.tensor([encoding[char] for char in seed_pattern])\n",
    "encoded_text = F.one_hot(encoded_text, num_classes=num_chars).to(torch.float)\n",
    "# Add a single batch dimension at the beginning\n",
    "encoded_text = encoded_text.unsqueeze(0)\n",
    "encoded_text = encoded_text.to(device)\n",
    "\n",
    "generated_text = \"\"\n",
    "for i in range(500):\n",
    "    # predict() gives a tensor of shape (1, 65) \n",
    "    # with 1 being the size of the batch, for that we use [0] to get a vector\n",
    "    output = predict(model, encoded_text)[0]\n",
    "    # Convert the output to probabilities\n",
    "    probs = torch.softmax(output, dim=-1)\n",
    "    # Randomly choose from a multinomial distribution with the output probabilities\n",
    "    # make the generation more diverse.\n",
    "    prediction = torch.multinomial(probs, num_samples=1)\n",
    "    generated_text += decoding[int(prediction)]\n",
    "    \n",
    "    # One hot encode the new (predicted) character\n",
    "    next_char_encoded = F.one_hot(prediction, num_classes=num_chars)\n",
    "    # Make sure it has a singular batch and seq_len dimension in order to concatenate them.\n",
    "    next_char_encoded = next_char_encoded.view(1, 1, num_chars)\n",
    "    # Remove first char and glue the predicted one to the end\n",
    "    encoded_text = torch.cat((encoded_text[:, 1:], next_char_encoded), dim=1)\n",
    "print(generated_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to many approach"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# chop up our data into X and y, slice into roughly \n",
    "# (num_chars / skip) overlapping 'sentences' of length \n",
    "# sentence_length, and encode the chars\n",
    "sentence_length = 20\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i+1:i+1 + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append([encoding[char] for char in next_char])\n",
    "\n",
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\"\n",
    "      .format(num_sentences, sentence_length))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(X_data[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print([decoding[idx] for idx in X_data[0]])\n",
    "print([decoding[idx] for idx in y_data[0]])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# One-hot encode the data.\n",
    "X = F.one_hot(torch.tensor(X_data), num_classes=num_chars).to(torch.float)\n",
    "y = torch.tensor(y_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# many2many model\n",
    "class mmRNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layer_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Defining the number of h layers and the nodes in each layer\n",
    "        self.layer_size = layer_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, layer_size, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        #out = out[:, -1] # as you can see this line was deleted: This means instead of taking just the last hidden\n",
    "        #state with out = out[:, -1] (size: batch_size x num_classes), we keep all the hidden states because out has \n",
    "        #a size of: batch_size x seq_len x num_classes (one hidden state per character in the sequence).\n",
    "\n",
    "        \n",
    "        #During inference, we take only the last hidden state\n",
    "        #because other predictions are used only to train the model\n",
    "        if not self.train:\n",
    "            out = out[:, -1]\n",
    "            \n",
    "        # the Linear layer works automatically with one more dimension.\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "hidden_size = 256\n",
    "seq_length = 20\n",
    "num_classes = 65\n",
    "layer_size = 1\n",
    "batch_size= 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelmm = mmRNNModel(num_classes, hidden_size, layer_size, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(modelmm.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "summary(modelmm, input_size=(batch_size, seq_length, num_classes))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train(epoch, model, data_loader, log_interval=200):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_correct = 0 \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(data_loader, desc=f\"Training Epoch {epoch}\")):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        output = model(data)\n",
    "        \n",
    "        #the only modification here consists in modifying the cross-entropy, because it takes inputs with \n",
    "        #dimensions: (N, C, d1, d2, ..., dk), that means the dimension of the classes is always the 2nd, \n",
    "        #and all the extra dimensions come at the end. You have to change your output to size:\n",
    "        # batch_size x seq_len x num_classes -> batch_size x num_classes x seq_len. \n",
    "        #It's just a transpose of the loss criterion (output.transpose(1, 2), target).\n",
    "        output = output.transpose(1, 2) \n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        _, pred = torch.max(output, dim=1)\n",
    "\n",
    "        total_correct += torch.sum(pred == target).item()\n",
    "                  \n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.data.item()))\n",
    "    \n",
    "    accuracy_train = total_correct / len(data_loader.dataset)\n",
    "\n",
    "    \n",
    "    total_train_loss = total_train_loss / len(data_loader)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_train_loss,\n",
    "        \"accuracy\": accuracy_train,\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "@torch.inference_mode() \n",
    "def predict(model, data):\n",
    "    # Put the model in eval mode\n",
    "    model.eval()\n",
    "    output = model(data)\n",
    "    return output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create dataloader from X,y tensors\n",
    "datasett = TensorDataset(X, y)\n",
    "train_loader = DataLoader(datasett, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# train the model\n",
    "# Keep track of stats to plot them\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_result = train(epoch, modelmm, train_loader)\n",
    "    train_losses.append(train_result[\"loss\"])\n",
    "    train_accuracies.append(train_result[\"accuracy\"])\n",
    "    \n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.legend()\n",
    "plt.grid()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def make_seed(seed_phrase=\"\"):\n",
    "        if seed_phrase:  \n",
    "            phrase_length = len(seed_phrase)\n",
    "            pattern = \"\"\n",
    "            for i in range (0, sentence_length):\n",
    "                pattern += seed_phrase[i % phrase_length]\n",
    "        else:            \n",
    "            seed = random.randint(0, corpus_length - sentence_length)\n",
    "            pattern = corpus[seed:seed + sentence_length]\n",
    "        return pattern\n",
    "\n",
    "seed_pattern = make_seed(\"In the early morning, the flower is shining\")\n",
    "\n",
    "# To Do as an optimization: add function here that takes as parameters the seed_pattern and the model\n",
    "\n",
    "encoded_text = torch.tensor([encoding[char] for char in seed_pattern])\n",
    "encoded_text = F.one_hot(encoded_text, num_classes=num_chars).to(torch.float)\n",
    "# Add a single batch dimension at the beginning\n",
    "encoded_text = encoded_text.unsqueeze(0)\n",
    "encoded_text = encoded_text.to(device)\n",
    "torch.manual_seed(42)\n",
    "generated_text = \"\"\n",
    "for i in range(500):\n",
    "    output = predict(modelmm, encoded_text)[0]\n",
    "    # Convert the output to probabilities\n",
    "    probs = torch.softmax(output[-1], dim=-1)\n",
    "    # make the generation more diverse.\n",
    "    prediction = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    generated_text += decoding[int(prediction)]\n",
    "    # One hot encode the new (predicted) character\n",
    "    next_char_encoded = F.one_hot(prediction, num_classes=num_chars)\n",
    "    # Make sure it has a singular batch and seq_len dimension in order to concatenate them.\n",
    "    next_char_encoded = next_char_encoded.view(1, 1, num_chars)\n",
    "    # Remove first char and glue the predicted one to the end\n",
    "    encoded_text = torch.cat((encoded_text[:, 1:], next_char_encoded), dim=1)\n",
    "print(generated_text)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
