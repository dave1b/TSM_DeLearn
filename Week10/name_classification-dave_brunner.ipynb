{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# G27\n",
    "Dave Brunner"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.820572Z",
     "start_time": "2024-04-29T19:30:13.519941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "The names can be found in text files in a src directory, one file per language.\n",
    "\n",
    "In the following you can find some utilities to load the data into pandas data frames. \n",
    "\n",
    "We will restrict to some common European languages. \n",
    "\n",
    "With the given selection, we will identify all the occurring characters and initialize an alphabet.<br>\n",
    "For this alphabet, we will use a one-hot-encoding to map them into a vector space representation. \n",
    "\n",
    "Foresee a suitable character for the end of the word, e.g. 'END'."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.822874Z",
     "start_time": "2024-04-29T19:30:14.821440Z"
    }
   },
   "source": [
    "srcdir = 'data/names'\n",
    "languages = [\"English\", \"French\", \"Italian\", \"German\", \"Spanish\"]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.824697Z",
     "start_time": "2024-04-29T19:30:14.823397Z"
    }
   },
   "source": [
    "# inspect the data directory\n",
    "def findFiles(path):\n",
    "    return glob.glob(path)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.827698Z",
     "start_time": "2024-04-29T19:30:14.825769Z"
    }
   },
   "source": "print('\\n'.join(findFiles(os.path.join(srcdir, '*.txt'))))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/names/Czech.txt\n",
      "data/names/German.txt\n",
      "data/names/Arabic.txt\n",
      "data/names/Japanese.txt\n",
      "data/names/Chinese.txt\n",
      "data/names/Vietnamese.txt\n",
      "data/names/Russian.txt\n",
      "data/names/French.txt\n",
      "data/names/Irish.txt\n",
      "data/names/English.txt\n",
      "data/names/Spanish.txt\n",
      "data/names/Greek.txt\n",
      "data/names/Italian.txt\n",
      "data/names/Portuguese.txt\n",
      "data/names/Scottish.txt\n",
      "data/names/Dutch.txt\n",
      "data/names/Korean.txt\n",
      "data/names/Polish.txt\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.830276Z",
     "start_time": "2024-04-29T19:30:14.828186Z"
    }
   },
   "source": [
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return lines\n",
    "\n",
    "\n",
    "def load_data(srcdir, categories=None):\n",
    "    names_list = []\n",
    "    for filename in findFiles(os.path.join(srcdir, '*.txt')):\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        if not categories or category in categories:\n",
    "            names = readLines(filename)\n",
    "            names_list.extend([(name, category) for name in names])\n",
    "    df = pd.DataFrame(names_list)\n",
    "    df.columns = [\"name\", \"lang\"]\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.837359Z",
     "start_time": "2024-04-29T19:30:14.830701Z"
    }
   },
   "source": [
    "names = load_data(srcdir, categories=languages)\n",
    "names.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       name    lang\n",
       "0    Abbing  German\n",
       "1      Abel  German\n",
       "2     Abeln  German\n",
       "3       Abt  German\n",
       "4  Achilles  German"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbing</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abel</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abeln</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abt</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Achilles</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.839896Z",
     "start_time": "2024-04-29T19:30:14.837972Z"
    }
   },
   "source": [
    "maxlen = np.max([len(name) for name in names.name])\n",
    "print(\"Maximum name length: \", maxlen)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum name length:  18\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.841712Z",
     "start_time": "2024-04-29T19:30:14.840358Z"
    }
   },
   "cell_type": "code",
   "source": "print(f'There are {len(names)} names in the dataset')",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5676 names in the dataset\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.844493Z",
     "start_time": "2024-04-29T19:30:14.842303Z"
    }
   },
   "source": [
    "alphabet = sorted(list(set(''.join([name for name in names.name]))))\n",
    "alphabet.append('END')\n",
    "len_alphabet = len(alphabet)\n",
    "char_index = dict((c, i) for i, c in enumerate(alphabet))\n",
    "print(\"Size of alphabet: \", len_alphabet)\n",
    "print(alphabet)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of alphabet:  74\n",
      "[' ', \"'\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Á', 'É', 'ß', 'à', 'á', 'ä', 'ç', 'è', 'é', 'ê', 'ì', 'í', 'ñ', 'ò', 'ó', 'ö', 'ù', 'ú', 'ü', 'END']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.848986Z",
     "start_time": "2024-04-29T19:30:14.846036Z"
    }
   },
   "source": "names.groupby('lang')['name'].count() / len(names)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang\n",
       "English    0.646230\n",
       "French     0.048802\n",
       "German     0.127555\n",
       "Italian    0.124912\n",
       "Spanish    0.052502\n",
       "Name: name, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Representations\n",
    "\n",
    "Now construct the vector representation by using one-hot-vectors. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.852237Z",
     "start_time": "2024-04-29T19:30:14.849448Z"
    }
   },
   "source": [
    "language_to_index = {country: index for index, country in enumerate(names.lang.unique())}\n",
    "index_to_language = {index: country for index, country in enumerate(names.lang.unique())}\n",
    "\n",
    "\n",
    "def onehot(i, length):\n",
    "    v = np.zeros(length);\n",
    "    v[i] = 1\n",
    "    return v\n",
    "\n",
    "\n",
    "def name_representation(name, maxlen):\n",
    "    name_trunc = str(name)[0:maxlen]\n",
    "    size = len(char_index)\n",
    "    vector = [onehot(char_index[j], size) for j in str(name)]\n",
    "    # fill the rest with \n",
    "    for k in range(0, maxlen - len(str(name))):\n",
    "        vector.append(onehot(char_index['END'], size))\n",
    "    return vector\n",
    "\n",
    "\n",
    "def lang_representation(language, language_to_index):\n",
    "    y = np.zeros(len(language_to_index))\n",
    "    y[language_to_index[language]] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def lang_from_output(score):\n",
    "    return index_to_language[np.argmax(score)]\n",
    "\n",
    "\n",
    "def predict(name, model):\n",
    "    score = model.predict(np.array([name_representation(name, maxlen)]))[0]\n",
    "    return lang_from_output(score)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train/test\n",
    "\n",
    "Split the data into train/test\n",
    "\n",
    "Shuffle the data\n",
    "\n",
    "Transform the names data into a suitable vector respresentation:\n",
    "* names into numpy arrays of shape (*,maxlen,len_alphabet)\n",
    "* language into numpy array of shape (*,len(languages))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.854276Z",
     "start_time": "2024-04-29T19:30:14.852734Z"
    }
   },
   "source": [
    "test_split = 0.2\n",
    "\n",
    "# Shuffle and split names data\n",
    "# names = names.sample(frac=1).reset_index(drop=True)\n",
    "# print(names.head())\n",
    "\n",
    "train = names[int(len(names) * test_split):]\n",
    "test = names[:int(len(names) * test_split)]"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.916034Z",
     "start_time": "2024-04-29T19:30:14.854775Z"
    }
   },
   "source": [
    "# Map train and test data into vector space (one-hot-vectors)\n",
    "X_train = np.array([name_representation(name, maxlen) for name in train.name])\n",
    "Y_train = np.array([lang_representation(lang, language_to_index) for lang in train.lang])\n",
    "\n",
    "X_test = np.array([name_representation(name, maxlen) for name in test.name])\n",
    "Y_test = np.array([lang_representation(lang, language_to_index) for lang in test.lang])"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:31:01.573937Z",
     "start_time": "2024-04-29T19:31:01.571476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_train[0].shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4541, 18, 74])\n",
      "torch.Size([1135, 18, 74])\n",
      "torch.Size([18, 74])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibly, pack the data into a Dataset (e.g. when working with in PyTorch)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:31:03.350429Z",
     "start_time": "2024-04-29T19:31:03.344856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert from numpy to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, Y_test)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ft/p6xwtlq12ygfhmnytl1ngxdm0000gn/T/ipykernel_25565/1765502775.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "/var/folders/ft/p6xwtlq12ygfhmnytl1ngxdm0000gn/T/ipykernel_25565/1765502775.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
      "/var/folders/ft/p6xwtlq12ygfhmnytl1ngxdm0000gn/T/ipykernel_25565/1765502775.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n",
      "/var/folders/ft/p6xwtlq12ygfhmnytl1ngxdm0000gn/T/ipykernel_25565/1765502775.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_test = torch.tensor(Y_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.930337Z",
     "start_time": "2024-04-29T19:30:14.928573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Train Model: Single Layer with SimpleRNN\n",
    "\n",
    "Create an RNN consisting of a single layer with a SimpleRNN and a softmax.\n",
    "\n",
    "Then train the model. Play with different number of hidden units in the layer to obtain a good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.935431Z",
     "start_time": "2024-04-29T19:30:14.931109Z"
    }
   },
   "source": [
    "from torch import nn\n",
    "\n",
    "input_size = len_alphabet\n",
    "hidden_size = 32\n",
    "output_size = len(languages)\n",
    "\n",
    "\n",
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, batch_first=True):\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                          batch_first=batch_first)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        print(h0.shape)\n",
    "        # Forward propagate RNN\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        # print(out.shape)\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return self.activation(out)\n",
    "\n",
    "\n",
    "model = ElmanRNN(input_size, hidden_size, output_size)\n",
    "model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElmanRNN(\n",
       "  (rnn): RNN(74, 32, batch_first=True)\n",
       "  (fc): Linear(in_features=32, out_features=5, bias=True)\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:14.937772Z",
     "start_time": "2024-04-29T19:30:14.936253Z"
    }
   },
   "source": [
    "# batch_size = 128\n",
    "# n_epochs = 10000\n",
    "# learning_rate = 0.1\n",
    "# model = ElmanRNN(input_size, hidden_size, output_size)\n",
    "# \n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "# \n",
    "# for epoch in range(n_epochs):\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for i, (names, labels) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "# \n",
    "#         outputs = model(names)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         \n",
    "#         # Get predictions from the maximum value\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         labels = torch.max(labels, 1)[1]\n",
    "# \n",
    "#         # Total number of labels\n",
    "#         total += labels.size(0)\n",
    "# \n",
    "#         # Total correct predictions\n",
    "#         correct += (predicted == labels).sum()\n",
    "#     \n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(\n",
    "#         f\"Epoch [{epoch + 1}/{n_epochs}], \"\n",
    "#         f\"Loss: {loss.item():.4f}, \"\n",
    "#         f\"Accuracy: {accuracy :.4f}\"\n",
    "#     )\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:18.581174Z",
     "start_time": "2024-04-29T19:30:14.938321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super(RNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.rnn = nn.RNN(input_size, hidden_size)\n",
    "    self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, x, hidden):\n",
    "    # print(x.shape)\n",
    "    output, hidden = self.rnn(x, hidden)\n",
    "    # print(output.shape)\n",
    "    output = self.linear(output)\n",
    "    # print(output.shape)\n",
    "    return output, hidden\n",
    "\n",
    "def transform_prediction(pred):\n",
    "    # Get the index of the maximum value in the tensor\n",
    "    max_index = torch.argmax(pred, dim=1)\n",
    "\n",
    "    # Create a new tensor filled with zeros of the same shape as pred\n",
    "    transformed_pred = torch.zeros_like(pred)\n",
    "\n",
    "    # Use scatter to set the index corresponding to the maximum value to one\n",
    "    transformed_pred.scatter_(1, max_index.unsqueeze(1), 1)\n",
    "    \n",
    "    return transformed_pred\n",
    "\n",
    "def train(model, data_loader, criterion, optimizer, num_epochs):\n",
    "  for epoch in range(num_epochs):\n",
    "    for names, labels in data_loader:\n",
    "      optimizer.zero_grad()\n",
    "      pred, h = model(names, None)\n",
    "      last_output = pred[:, -1, :]\n",
    "      last_output = transform_prediction(last_output)\n",
    "      \n",
    "      loss = criterion(last_output, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    # Print training progress (optional)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Example usage (replace with your data loaders)\n",
    "input_size = len_alphabet\n",
    "hidden_size = 5\n",
    "output_size = len(languages)\n",
    "model = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  \n",
    "\n",
    "train(model, train_loader, criterion, optimizer, num_epochs=100)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.6917\n",
      "Epoch [2/100], Loss: 1.7245\n",
      "Epoch [3/100], Loss: 1.7573\n",
      "Epoch [4/100], Loss: 1.8065\n",
      "Epoch [5/100], Loss: 1.6917\n",
      "Epoch [6/100], Loss: 1.7081\n",
      "Epoch [7/100], Loss: 1.7901\n",
      "Epoch [8/100], Loss: 1.7737\n",
      "Epoch [9/100], Loss: 1.7737\n",
      "Epoch [10/100], Loss: 1.7245\n",
      "Epoch [11/100], Loss: 1.8229\n",
      "Epoch [12/100], Loss: 1.6753\n",
      "Epoch [13/100], Loss: 1.7737\n",
      "Epoch [14/100], Loss: 1.7409\n",
      "Epoch [15/100], Loss: 1.6589\n",
      "Epoch [16/100], Loss: 1.6261\n",
      "Epoch [17/100], Loss: 1.7901\n",
      "Epoch [18/100], Loss: 1.7409\n",
      "Epoch [19/100], Loss: 1.8393\n",
      "Epoch [20/100], Loss: 1.7901\n",
      "Epoch [21/100], Loss: 1.6589\n",
      "Epoch [22/100], Loss: 1.7573\n",
      "Epoch [23/100], Loss: 1.7737\n",
      "Epoch [24/100], Loss: 1.7737\n",
      "Epoch [25/100], Loss: 1.8393\n",
      "Epoch [26/100], Loss: 1.7245\n",
      "Epoch [27/100], Loss: 1.6917\n",
      "Epoch [28/100], Loss: 1.6753\n",
      "Epoch [29/100], Loss: 1.7409\n",
      "Epoch [30/100], Loss: 1.8065\n",
      "Epoch [31/100], Loss: 1.7901\n",
      "Epoch [32/100], Loss: 1.7573\n",
      "Epoch [33/100], Loss: 1.7409\n",
      "Epoch [34/100], Loss: 1.7245\n",
      "Epoch [35/100], Loss: 1.6753\n",
      "Epoch [36/100], Loss: 1.7737\n",
      "Epoch [37/100], Loss: 1.7081\n",
      "Epoch [38/100], Loss: 1.8065\n",
      "Epoch [39/100], Loss: 1.8065\n",
      "Epoch [40/100], Loss: 1.7737\n",
      "Epoch [41/100], Loss: 1.7409\n",
      "Epoch [42/100], Loss: 1.7081\n",
      "Epoch [43/100], Loss: 1.7737\n",
      "Epoch [44/100], Loss: 1.7737\n",
      "Epoch [45/100], Loss: 1.8065\n",
      "Epoch [46/100], Loss: 1.7737\n",
      "Epoch [47/100], Loss: 1.7737\n",
      "Epoch [48/100], Loss: 1.7081\n",
      "Epoch [49/100], Loss: 1.6589\n",
      "Epoch [50/100], Loss: 1.7573\n",
      "Epoch [51/100], Loss: 1.6589\n",
      "Epoch [52/100], Loss: 1.7245\n",
      "Epoch [53/100], Loss: 1.7573\n",
      "Epoch [54/100], Loss: 1.8557\n",
      "Epoch [55/100], Loss: 1.7409\n",
      "Epoch [56/100], Loss: 1.7737\n",
      "Epoch [57/100], Loss: 1.7409\n",
      "Epoch [58/100], Loss: 1.7245\n",
      "Epoch [59/100], Loss: 1.8229\n",
      "Epoch [60/100], Loss: 1.7081\n",
      "Epoch [61/100], Loss: 1.6425\n",
      "Epoch [62/100], Loss: 1.7409\n",
      "Epoch [63/100], Loss: 1.7409\n",
      "Epoch [64/100], Loss: 1.7573\n",
      "Epoch [65/100], Loss: 1.8229\n",
      "Epoch [66/100], Loss: 1.7901\n",
      "Epoch [67/100], Loss: 1.8065\n",
      "Epoch [68/100], Loss: 1.7409\n",
      "Epoch [69/100], Loss: 1.7409\n",
      "Epoch [70/100], Loss: 1.8065\n",
      "Epoch [71/100], Loss: 1.7409\n",
      "Epoch [72/100], Loss: 1.7409\n",
      "Epoch [73/100], Loss: 1.8065\n",
      "Epoch [74/100], Loss: 1.6917\n",
      "Epoch [75/100], Loss: 1.7081\n",
      "Epoch [76/100], Loss: 1.7409\n",
      "Epoch [77/100], Loss: 1.7737\n",
      "Epoch [78/100], Loss: 1.6917\n",
      "Epoch [79/100], Loss: 1.7409\n",
      "Epoch [80/100], Loss: 1.7737\n",
      "Epoch [81/100], Loss: 1.7409\n",
      "Epoch [82/100], Loss: 1.7409\n",
      "Epoch [83/100], Loss: 1.8229\n",
      "Epoch [84/100], Loss: 1.7081\n",
      "Epoch [85/100], Loss: 1.7409\n",
      "Epoch [86/100], Loss: 1.7737\n",
      "Epoch [87/100], Loss: 1.7901\n",
      "Epoch [88/100], Loss: 1.8229\n",
      "Epoch [89/100], Loss: 1.7901\n",
      "Epoch [90/100], Loss: 1.8065\n",
      "Epoch [91/100], Loss: 1.7901\n",
      "Epoch [92/100], Loss: 1.7737\n",
      "Epoch [93/100], Loss: 1.7409\n",
      "Epoch [94/100], Loss: 1.7573\n",
      "Epoch [95/100], Loss: 1.7245\n",
      "Epoch [96/100], Loss: 1.8229\n",
      "Epoch [97/100], Loss: 1.7573\n",
      "Epoch [98/100], Loss: 1.7245\n",
      "Epoch [99/100], Loss: 1.7737\n",
      "Epoch [100/100], Loss: 1.7409\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Model with several SimpleRNN Layers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:18.583133Z",
     "start_time": "2024-04-29T19:30:18.581715Z"
    }
   },
   "source": [
    "### START YOUR CODE\n",
    "\n",
    "model = ...\n",
    "\n",
    "### END YOUR CODE"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:18.586085Z",
     "start_time": "2024-04-29T19:30:18.583772Z"
    }
   },
   "source": [
    "### START YOUR CODE\n",
    "\n",
    "# train...\n",
    "\n",
    "### END YOUR CODE"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Handling\n",
    "\n",
    "Choose a method to address the class imbalance seen in the given example.\n",
    "- minority resampling \n",
    "- class weights in the loss\n",
    "\n",
    "Implement it and incorporate it in the training.\n",
    "Evaluate the results and compare it with the results obtained with the unbalanced training.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:30:18.587875Z",
     "start_time": "2024-04-29T19:30:18.586714Z"
    }
   },
   "source": [
    "### START YOUR CODE\n",
    "\n",
    "# train...\n",
    "\n",
    "### END YOUR CODE"
   ],
   "outputs": [],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
