{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# G27\n",
    "Dave Brunner"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.669445Z",
     "start_time": "2024-04-29T17:42:11.667553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "The names can be found in text files in a src directory, one file per language.\n",
    "\n",
    "In the following you can find some utilities to load the data into pandas data frames. \n",
    "\n",
    "We will restrict to some common European languages. \n",
    "\n",
    "With the given selection, we will identify all the occurring characters and initialize an alphabet.<br>\n",
    "For this alphabet, we will use a one-hot-encoding to map them into a vector space representation. \n",
    "\n",
    "Foresee a suitable character for the end of the word, e.g. 'END'."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.705800Z",
     "start_time": "2024-04-29T17:42:11.704083Z"
    }
   },
   "source": [
    "srcdir = 'data/names'\n",
    "languages = [\"English\", \"French\", \"Italian\", \"German\", \"Spanish\"]"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.708392Z",
     "start_time": "2024-04-29T17:42:11.707104Z"
    }
   },
   "source": [
    "# inspect the data directory\n",
    "def findFiles(path):\n",
    "    return glob.glob(path)"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.717413Z",
     "start_time": "2024-04-29T17:42:11.715110Z"
    }
   },
   "source": "print('\\n'.join(findFiles(os.path.join(srcdir, '*.txt'))))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/names/Czech.txt\n",
      "data/names/German.txt\n",
      "data/names/Arabic.txt\n",
      "data/names/Japanese.txt\n",
      "data/names/Chinese.txt\n",
      "data/names/Vietnamese.txt\n",
      "data/names/Russian.txt\n",
      "data/names/French.txt\n",
      "data/names/Irish.txt\n",
      "data/names/English.txt\n",
      "data/names/Spanish.txt\n",
      "data/names/Greek.txt\n",
      "data/names/Italian.txt\n",
      "data/names/Portuguese.txt\n",
      "data/names/Scottish.txt\n",
      "data/names/Dutch.txt\n",
      "data/names/Korean.txt\n",
      "data/names/Polish.txt\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.720192Z",
     "start_time": "2024-04-29T17:42:11.718117Z"
    }
   },
   "source": [
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return lines\n",
    "\n",
    "\n",
    "def load_data(srcdir, categories=None):\n",
    "    names_list = []\n",
    "    for filename in findFiles(os.path.join(srcdir, '*.txt')):\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        if not categories or category in categories:\n",
    "            names = readLines(filename)\n",
    "            names_list.extend([(name, category) for name in names])\n",
    "    df = pd.DataFrame(names_list)\n",
    "    df.columns = [\"name\", \"lang\"]\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.730225Z",
     "start_time": "2024-04-29T17:42:11.725520Z"
    }
   },
   "source": [
    "names = load_data(srcdir, categories=languages)\n",
    "names.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       name    lang\n",
       "0    Abbing  German\n",
       "1      Abel  German\n",
       "2     Abeln  German\n",
       "3       Abt  German\n",
       "4  Achilles  German"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbing</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abel</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abeln</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abt</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Achilles</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.733011Z",
     "start_time": "2024-04-29T17:42:11.730900Z"
    }
   },
   "source": [
    "maxlen = np.max([len(name) for name in names.name])\n",
    "print(\"Maximum name length: \", maxlen)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum name length:  18\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.738111Z",
     "start_time": "2024-04-29T17:42:11.736679Z"
    }
   },
   "cell_type": "code",
   "source": "print(f'There are {len(names)} names in the dataset')",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5676 names in the dataset\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.741653Z",
     "start_time": "2024-04-29T17:42:11.739389Z"
    }
   },
   "source": [
    "alphabet = sorted(list(set(''.join([name for name in names.name]))))\n",
    "alphabet.append('END')\n",
    "len_alphabet = len(alphabet)\n",
    "char_index = dict((c, i) for i, c in enumerate(alphabet))\n",
    "print(\"Size of alphabet: \", len_alphabet)\n",
    "print(alphabet)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of alphabet:  74\n",
      "[' ', \"'\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Á', 'É', 'ß', 'à', 'á', 'ä', 'ç', 'è', 'é', 'ê', 'ì', 'í', 'ñ', 'ò', 'ó', 'ö', 'ù', 'ú', 'ü', 'END']\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.751542Z",
     "start_time": "2024-04-29T17:42:11.748281Z"
    }
   },
   "source": "names.groupby('lang')['name'].count() / len(names)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang\n",
       "English    0.646230\n",
       "French     0.048802\n",
       "German     0.127555\n",
       "Italian    0.124912\n",
       "Spanish    0.052502\n",
       "Name: name, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Representations\n",
    "\n",
    "Now construct the vector representation by using one-hot-vectors. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.755682Z",
     "start_time": "2024-04-29T17:42:11.752312Z"
    }
   },
   "source": [
    "language_to_index = {country: index for index, country in enumerate(names.lang.unique())}\n",
    "index_to_language = {index: country for index, country in enumerate(names.lang.unique())}\n",
    "\n",
    "\n",
    "def onehot(i, length):\n",
    "    v = np.zeros(length);\n",
    "    v[i] = 1\n",
    "    return v\n",
    "\n",
    "\n",
    "def name_representation(name, maxlen):\n",
    "    name_trunc = str(name)[0:maxlen]\n",
    "    size = len(char_index)\n",
    "    vector = [onehot(char_index[j], size) for j in str(name)]\n",
    "    # fill the rest with \n",
    "    for k in range(0, maxlen - len(str(name))):\n",
    "        vector.append(onehot(char_index['END'], size))\n",
    "    return vector\n",
    "\n",
    "\n",
    "def lang_representation(language, language_to_index):\n",
    "    y = np.zeros(len(language_to_index))\n",
    "    y[language_to_index[language]] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def lang_from_output(score):\n",
    "    return index_to_language[np.argmax(score)]\n",
    "\n",
    "\n",
    "def predict(name, model):\n",
    "    score = model.predict(np.array([name_representation(name, maxlen)]))[0]\n",
    "    return lang_from_output(score)"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train/test\n",
    "\n",
    "Split the data into train/test\n",
    "\n",
    "Shuffle the data\n",
    "\n",
    "Transform the names data into a suitable vector respresentation:\n",
    "* names into numpy arrays of shape (*,maxlen,len_alphabet)\n",
    "* language into numpy array of shape (*,len(languages))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.759898Z",
     "start_time": "2024-04-29T17:42:11.756699Z"
    }
   },
   "source": [
    "test_split = 0.2\n",
    "\n",
    "# Shuffle and split names data\n",
    "names = names.sample(frac=1).reset_index(drop=True)\n",
    "print(names.head())\n",
    "\n",
    "train = names[int(len(names) * test_split):]\n",
    "test = names[:int(len(names) * test_split)]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name     lang\n",
      "0     Gosse   French\n",
      "1    Beadle  English\n",
      "2  Thirlway  English\n",
      "3     Masin  Italian\n",
      "4     Batty  English\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.822505Z",
     "start_time": "2024-04-29T17:42:11.760626Z"
    }
   },
   "source": [
    "# Map train and test data into vector space (one-hot-vectors)\n",
    "X_train = np.array([name_representation(name, maxlen) for name in train.name])\n",
    "Y_train = np.array([lang_representation(lang, language_to_index) for lang in train.lang])\n",
    "\n",
    "X_test = np.array([name_representation(name, maxlen) for name in test.name])\n",
    "Y_test = np.array([lang_representation(lang, language_to_index) for lang in test.lang])"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.825046Z",
     "start_time": "2024-04-29T17:42:11.823428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4541, 18, 74)\n",
      "(1135, 18, 74)\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibly, pack the data into a Dataset (e.g. when working with in PyTorch)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.831394Z",
     "start_time": "2024-04-29T17:42:11.825768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert from numpy to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, Y_test)"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:11.835016Z",
     "start_time": "2024-04-29T17:42:11.832903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Train Model: Single Layer with SimpleRNN\n",
    "\n",
    "Create an RNN consisting of a single layer with a SimpleRNN and a softmax.\n",
    "\n",
    "Then train the model. Play with different number of hidden units in the layer to obtain a good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-29T17:43:04.463763Z",
     "start_time": "2024-04-29T17:43:04.460236Z"
    }
   },
   "source": [
    "from torch import nn\n",
    "\n",
    "input_size = len_alphabet\n",
    "hidden_size = 32\n",
    "output_size = len(languages)\n",
    "\n",
    "\n",
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, batch_first=True):\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                          batch_first=batch_first)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "\n",
    "        # print(h0.shape)\n",
    "        # Forward propagate RNN\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        # print(out.shape)\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return self.activation(out)\n",
    "\n",
    "\n",
    "model = ElmanRNN(input_size, hidden_size, output_size)\n",
    "model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElmanRNN(\n",
       "  (rnn): RNN(74, 32, batch_first=True)\n",
       "  (fc): Linear(in_features=32, out_features=5, bias=True)\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:43:09.344643Z",
     "start_time": "2024-04-29T17:43:04.625426Z"
    }
   },
   "source": [
    "batch_size = 128\n",
    "n_epochs = 10000\n",
    "learning_rate = 0.1\n",
    "model = ElmanRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (names, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(names)\n",
    "        # print(output.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        labels = torch.max(labels, 1)[1]\n",
    "\n",
    "        # Total number of labels\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Total correct predictions\n",
    "        correct += (predicted == labels).sum()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(\n",
    "        f\"Epoch [{epoch + 1}/{n_epochs}], \"\n",
    "        f\"Loss: {loss.item():.4f}, \"\n",
    "        f\"Accuracy: {accuracy :.4f}\"\n",
    "    )\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Loss: 1.3813, Accuracy: 62.9817\n",
      "Epoch [2/10000], Loss: 1.3864, Accuracy: 64.5673\n",
      "Epoch [3/10000], Loss: 1.1726, Accuracy: 64.5673\n",
      "Epoch [4/10000], Loss: 1.1343, Accuracy: 64.5673\n",
      "Epoch [5/10000], Loss: 1.3216, Accuracy: 64.5673\n",
      "Epoch [6/10000], Loss: 1.2457, Accuracy: 64.5673\n",
      "Epoch [7/10000], Loss: 1.2106, Accuracy: 64.5673\n",
      "Epoch [8/10000], Loss: 1.0455, Accuracy: 64.5673\n",
      "Epoch [9/10000], Loss: 1.0321, Accuracy: 64.5673\n",
      "Epoch [10/10000], Loss: 1.1444, Accuracy: 64.5673\n",
      "Epoch [11/10000], Loss: 1.1253, Accuracy: 64.5673\n",
      "Epoch [12/10000], Loss: 1.1364, Accuracy: 64.5673\n",
      "Epoch [13/10000], Loss: 1.1217, Accuracy: 64.5673\n",
      "Epoch [14/10000], Loss: 1.4466, Accuracy: 64.5673\n",
      "Epoch [15/10000], Loss: 1.0717, Accuracy: 64.5673\n",
      "Epoch [16/10000], Loss: 1.1897, Accuracy: 64.5673\n",
      "Epoch [17/10000], Loss: 0.9903, Accuracy: 64.5673\n",
      "Epoch [18/10000], Loss: 1.2248, Accuracy: 64.5673\n",
      "Epoch [19/10000], Loss: 1.2162, Accuracy: 64.5673\n",
      "Epoch [20/10000], Loss: 1.0872, Accuracy: 64.5673\n",
      "Epoch [21/10000], Loss: 1.0562, Accuracy: 64.5673\n",
      "Epoch [22/10000], Loss: 1.0260, Accuracy: 64.5673\n",
      "Epoch [23/10000], Loss: 1.0772, Accuracy: 64.5673\n",
      "Epoch [24/10000], Loss: 1.2732, Accuracy: 64.5673\n",
      "Epoch [25/10000], Loss: 1.2848, Accuracy: 64.5673\n",
      "Epoch [26/10000], Loss: 1.0175, Accuracy: 64.5673\n",
      "Epoch [27/10000], Loss: 1.1569, Accuracy: 64.5673\n",
      "Epoch [28/10000], Loss: 1.0876, Accuracy: 64.5673\n",
      "Epoch [29/10000], Loss: 1.1267, Accuracy: 64.5673\n",
      "Epoch [30/10000], Loss: 1.3002, Accuracy: 64.5673\n",
      "Epoch [31/10000], Loss: 1.2839, Accuracy: 64.5673\n",
      "Epoch [32/10000], Loss: 0.9539, Accuracy: 64.5673\n",
      "Epoch [33/10000], Loss: 1.2752, Accuracy: 64.5673\n",
      "Epoch [34/10000], Loss: 1.0878, Accuracy: 64.5673\n",
      "Epoch [35/10000], Loss: 1.2160, Accuracy: 64.5673\n",
      "Epoch [36/10000], Loss: 1.1091, Accuracy: 64.5673\n",
      "Epoch [37/10000], Loss: 1.0589, Accuracy: 64.5673\n",
      "Epoch [38/10000], Loss: 1.2522, Accuracy: 64.5673\n",
      "Epoch [39/10000], Loss: 1.1911, Accuracy: 64.5673\n",
      "Epoch [40/10000], Loss: 1.3628, Accuracy: 64.5673\n",
      "Epoch [41/10000], Loss: 1.1022, Accuracy: 64.5673\n",
      "Epoch [42/10000], Loss: 1.3765, Accuracy: 64.5673\n",
      "Epoch [43/10000], Loss: 1.2381, Accuracy: 64.5673\n",
      "Epoch [44/10000], Loss: 1.4293, Accuracy: 64.5673\n",
      "Epoch [45/10000], Loss: 0.9299, Accuracy: 64.5673\n",
      "Epoch [46/10000], Loss: 1.1442, Accuracy: 64.5673\n",
      "Epoch [47/10000], Loss: 1.0913, Accuracy: 64.5673\n",
      "Epoch [48/10000], Loss: 1.0182, Accuracy: 64.5673\n",
      "Epoch [49/10000], Loss: 1.0912, Accuracy: 64.5673\n",
      "Epoch [50/10000], Loss: 1.3643, Accuracy: 64.5673\n",
      "Epoch [51/10000], Loss: 0.9762, Accuracy: 64.5673\n",
      "Epoch [52/10000], Loss: 1.1449, Accuracy: 64.5673\n",
      "Epoch [53/10000], Loss: 1.1538, Accuracy: 64.5673\n",
      "Epoch [54/10000], Loss: 1.4012, Accuracy: 64.5673\n",
      "Epoch [55/10000], Loss: 1.1550, Accuracy: 64.5673\n",
      "Epoch [56/10000], Loss: 0.9826, Accuracy: 64.5673\n",
      "Epoch [57/10000], Loss: 0.9820, Accuracy: 64.5673\n",
      "Epoch [58/10000], Loss: 1.2459, Accuracy: 64.5673\n",
      "Epoch [59/10000], Loss: 1.2454, Accuracy: 64.5673\n",
      "Epoch [60/10000], Loss: 1.1912, Accuracy: 64.5673\n",
      "Epoch [61/10000], Loss: 1.0162, Accuracy: 64.5673\n",
      "Epoch [62/10000], Loss: 1.1371, Accuracy: 64.5673\n",
      "Epoch [63/10000], Loss: 0.9928, Accuracy: 64.5673\n",
      "Epoch [64/10000], Loss: 1.1589, Accuracy: 64.5673\n",
      "Epoch [65/10000], Loss: 1.1739, Accuracy: 64.5673\n",
      "Epoch [66/10000], Loss: 1.0575, Accuracy: 64.5673\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[78], line 23\u001B[0m\n\u001B[1;32m     19\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     21\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 23\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Get predictions from the maximum value\u001B[39;00m\n\u001B[1;32m     26\u001B[0m _, predicted \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs\u001B[38;5;241m.\u001B[39mdata, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    381\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    382\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    383\u001B[0m             )\n\u001B[0;32m--> 385\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    388\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/optim/adam.py:166\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    155\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    157\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    158\u001B[0m         group,\n\u001B[1;32m    159\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    163\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    164\u001B[0m         state_steps)\n\u001B[0;32m--> 166\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/optim/adam.py:316\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    314\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 316\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    320\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    321\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    322\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    323\u001B[0m \u001B[43m     \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    324\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    326\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    327\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    328\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    330\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    331\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    332\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/optim/adam.py:439\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    437\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (max_exp_avg_sqs[i]\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 439\u001B[0m         denom \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbias_correction2_sqrt\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_\u001B[49m\u001B[43m(\u001B[49m\u001B[43meps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    441\u001B[0m     param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n\u001B[1;32m    443\u001B[0m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Model with several SimpleRNN Layers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-29T17:42:16.056041Z",
     "start_time": "2024-04-29T17:42:16.055996Z"
    }
   },
   "source": [
    "### START YOUR CODE\n",
    "\n",
    "model = ...\n",
    "\n",
    "### END YOUR CODE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### START YOUR CODE\n",
    "\n",
    "# train...\n",
    "\n",
    "### END YOUR CODE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Handling\n",
    "\n",
    "Choose a method to address the class imbalance seen in the given example.\n",
    "- minority resampling \n",
    "- class weights in the loss\n",
    "\n",
    "Implement it and incorporate it in the training.\n",
    "Evaluate the results and compare it with the results obtained with the unbalanced training.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### START YOUR CODE\n",
    "\n",
    "# train...\n",
    "\n",
    "### END YOUR CODE"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
