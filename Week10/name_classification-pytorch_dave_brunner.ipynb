{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# G27\n",
    "Dave Brunner"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.693375Z",
     "start_time": "2024-04-29T19:35:28.691472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "The names can be found in text files in a src directory, one file per language.\n",
    "\n",
    "In the following you can find some utilities to load the data into pandas data frames. \n",
    "\n",
    "We will restrict to some common European languages. \n",
    "\n",
    "With the given selection, we will identify all the occurring characters and initialize an alphabet.<br>\n",
    "For this alphabet, we will use a one-hot-encoding to map them into a vector space representation. \n",
    "\n",
    "Foresee a suitable character for the end of the word, e.g. 'END'."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.735890Z",
     "start_time": "2024-04-29T19:35:28.734247Z"
    }
   },
   "source": [
    "srcdir = 'data/names'\n",
    "languages = [\"English\", \"French\", \"Italian\", \"German\", \"Spanish\"]"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.738311Z",
     "start_time": "2024-04-29T19:35:28.736861Z"
    }
   },
   "source": [
    "# inspect the data directory\n",
    "def findFiles(path):\n",
    "    return glob.glob(path)"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.749013Z",
     "start_time": "2024-04-29T19:35:28.746727Z"
    }
   },
   "source": "print('\\n'.join(findFiles(os.path.join(srcdir, '*.txt'))))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/names/Czech.txt\n",
      "data/names/German.txt\n",
      "data/names/Arabic.txt\n",
      "data/names/Japanese.txt\n",
      "data/names/Chinese.txt\n",
      "data/names/Vietnamese.txt\n",
      "data/names/Russian.txt\n",
      "data/names/French.txt\n",
      "data/names/Irish.txt\n",
      "data/names/English.txt\n",
      "data/names/Spanish.txt\n",
      "data/names/Greek.txt\n",
      "data/names/Italian.txt\n",
      "data/names/Portuguese.txt\n",
      "data/names/Scottish.txt\n",
      "data/names/Dutch.txt\n",
      "data/names/Korean.txt\n",
      "data/names/Polish.txt\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.751646Z",
     "start_time": "2024-04-29T19:35:28.749683Z"
    }
   },
   "source": [
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return lines\n",
    "\n",
    "\n",
    "def load_data(srcdir, categories=None):\n",
    "    names_list = []\n",
    "    for filename in findFiles(os.path.join(srcdir, '*.txt')):\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        if not categories or category in categories:\n",
    "            names = readLines(filename)\n",
    "            names_list.extend([(name, category) for name in names])\n",
    "    df = pd.DataFrame(names_list)\n",
    "    df.columns = [\"name\", \"lang\"]\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.758694Z",
     "start_time": "2024-04-29T19:35:28.752320Z"
    }
   },
   "source": [
    "names = load_data(srcdir, categories=languages)\n",
    "names.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       name    lang\n",
       "0    Abbing  German\n",
       "1      Abel  German\n",
       "2     Abeln  German\n",
       "3       Abt  German\n",
       "4  Achilles  German"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbing</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abel</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abeln</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abt</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Achilles</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.766553Z",
     "start_time": "2024-04-29T19:35:28.764402Z"
    }
   },
   "source": [
    "maxlen = np.max([len(name) for name in names.name])\n",
    "print(\"Maximum name length: \", maxlen)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum name length:  18\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.774690Z",
     "start_time": "2024-04-29T19:35:28.773232Z"
    }
   },
   "cell_type": "code",
   "source": "print(f'There are {len(names)} names in the dataset')",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5676 names in the dataset\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.777901Z",
     "start_time": "2024-04-29T19:35:28.775728Z"
    }
   },
   "source": [
    "alphabet = sorted(list(set(''.join([name for name in names.name]))))\n",
    "alphabet.append('END')\n",
    "len_alphabet = len(alphabet)\n",
    "char_index = dict((c, i) for i, c in enumerate(alphabet))\n",
    "print(\"Size of alphabet: \", len_alphabet)\n",
    "print(alphabet)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of alphabet:  74\n",
      "[' ', \"'\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Á', 'É', 'ß', 'à', 'á', 'ä', 'ç', 'è', 'é', 'ê', 'ì', 'í', 'ñ', 'ò', 'ó', 'ö', 'ù', 'ú', 'ü', 'END']\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.781844Z",
     "start_time": "2024-04-29T19:35:28.778556Z"
    }
   },
   "source": "names.groupby('lang')['name'].count() / len(names)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang\n",
       "English    0.646230\n",
       "French     0.048802\n",
       "German     0.127555\n",
       "Italian    0.124912\n",
       "Spanish    0.052502\n",
       "Name: name, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Representations\n",
    "\n",
    "Now construct the vector representation by using one-hot-vectors. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.790989Z",
     "start_time": "2024-04-29T19:35:28.788128Z"
    }
   },
   "source": [
    "language_to_index = {country: index for index, country in enumerate(names.lang.unique())}\n",
    "index_to_language = {index: country for index, country in enumerate(names.lang.unique())}\n",
    "\n",
    "\n",
    "def onehot(i, length):\n",
    "    v = np.zeros(length);\n",
    "    v[i] = 1\n",
    "    return v\n",
    "\n",
    "\n",
    "def name_representation(name, maxlen):\n",
    "    name_trunc = str(name)[0:maxlen]\n",
    "    size = len(char_index)\n",
    "    vector = [onehot(char_index[j], size) for j in str(name)]\n",
    "    # fill the rest with \n",
    "    for k in range(0, maxlen - len(str(name))):\n",
    "        vector.append(onehot(char_index['END'], size))\n",
    "    return vector\n",
    "\n",
    "\n",
    "def lang_representation(language, language_to_index):\n",
    "    y = np.zeros(len(language_to_index))\n",
    "    y[language_to_index[language]] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def lang_from_output(score):\n",
    "    return index_to_language[np.argmax(score)]\n",
    "\n",
    "\n",
    "def predict(name, model):\n",
    "    score = model.predict(np.array([name_representation(name, maxlen)]))[0]\n",
    "    return lang_from_output(score)"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train/test\n",
    "\n",
    "Split the data into train/test\n",
    "\n",
    "Shuffle the data\n",
    "\n",
    "Transform the names data into a suitable vector respresentation:\n",
    "* names into numpy arrays of shape (*,maxlen,len_alphabet)\n",
    "* language into numpy array of shape (*,len(languages))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.793330Z",
     "start_time": "2024-04-29T19:35:28.791745Z"
    }
   },
   "source": [
    "test_split = 0.2\n",
    "\n",
    "# Shuffle and split names data\n",
    "# names = names.sample(frac=1).reset_index(drop=True)\n",
    "# print(names.head())\n",
    "\n",
    "train = names[int(len(names) * test_split):]\n",
    "test = names[:int(len(names) * test_split)]"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.858005Z",
     "start_time": "2024-04-29T19:35:28.798372Z"
    }
   },
   "source": [
    "# Map train and test data into vector space (one-hot-vectors)\n",
    "X_train = np.array([name_representation(name, maxlen) for name in train.name])\n",
    "Y_train = np.array([lang_representation(lang, language_to_index) for lang in train.lang])\n",
    "\n",
    "X_test = np.array([name_representation(name, maxlen) for name in test.name])\n",
    "Y_test = np.array([lang_representation(lang, language_to_index) for lang in test.lang])"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.860539Z",
     "start_time": "2024-04-29T19:35:28.858957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_train[0].shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4541, 18, 74)\n",
      "(1135, 18, 74)\n",
      "(18, 74)\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibly, pack the data into a Dataset (e.g. when working with in PyTorch)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.867859Z",
     "start_time": "2024-04-29T19:35:28.861205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert from numpy to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, Y_test)"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.872640Z",
     "start_time": "2024-04-29T19:35:28.870117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Train Model: Single Layer with SimpleRNN\n",
    "\n",
    "Create an RNN consisting of a single layer with a SimpleRNN and a softmax.\n",
    "\n",
    "Then train the model. Play with different number of hidden units in the layer to obtain a good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.877876Z",
     "start_time": "2024-04-29T19:35:28.873227Z"
    }
   },
   "source": [
    "from torch import nn\n",
    "\n",
    "input_size = len_alphabet\n",
    "hidden_size = 32\n",
    "output_size = len(languages)\n",
    "\n",
    "\n",
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, batch_first=True):\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                          batch_first=batch_first)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        print(h0.shape)\n",
    "        # Forward propagate RNN\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        # print(out.shape)\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return self.activation(out)\n",
    "\n",
    "\n",
    "model = ElmanRNN(input_size, hidden_size, output_size)\n",
    "model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElmanRNN(\n",
       "  (rnn): RNN(74, 32, batch_first=True)\n",
       "  (fc): Linear(in_features=32, out_features=5, bias=True)\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.880511Z",
     "start_time": "2024-04-29T19:35:28.878491Z"
    }
   },
   "source": [
    "# batch_size = 128\n",
    "# n_epochs = 10000\n",
    "# learning_rate = 0.1\n",
    "# model = ElmanRNN(input_size, hidden_size, output_size)\n",
    "# \n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "# \n",
    "# for epoch in range(n_epochs):\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for i, (names, labels) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "# \n",
    "#         outputs = model(names)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         \n",
    "#         # Get predictions from the maximum value\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         labels = torch.max(labels, 1)[1]\n",
    "# \n",
    "#         # Total number of labels\n",
    "#         total += labels.size(0)\n",
    "# \n",
    "#         # Total correct predictions\n",
    "#         correct += (predicted == labels).sum()\n",
    "#     \n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(\n",
    "#         f\"Epoch [{epoch + 1}/{n_epochs}], \"\n",
    "#         f\"Loss: {loss.item():.4f}, \"\n",
    "#         f\"Accuracy: {accuracy :.4f}\"\n",
    "#     )\n"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:35:28.901747Z",
     "start_time": "2024-04-29T19:35:28.881715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super(RNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.rnn = nn.RNN(input_size, hidden_size)\n",
    "    self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, x, hidden):\n",
    "    # print(x.shape)\n",
    "    output, hidden = self.rnn(x, hidden)\n",
    "    # print(output.shape)\n",
    "    output = self.linear(output)\n",
    "    # print(output.shape)\n",
    "    return output, hidden\n",
    "\n",
    "def transform_prediction(pred):\n",
    "    # Get the index of the maximum value in the tensor\n",
    "    max_index = torch.argmax(pred, dim=1)\n",
    "\n",
    "    # Create a new tensor filled with zeros of the same shape as pred\n",
    "    transformed_pred = torch.zeros_like(pred)\n",
    "\n",
    "    # Use scatter to set the index corresponding to the maximum value to one\n",
    "    transformed_pred.scatter_(1, max_index.unsqueeze(1), 1)\n",
    "    # enable gradient\n",
    "    \n",
    "    return transformed_pred\n",
    "\n",
    "def train(model, data_loader, criterion, optimizer, num_epochs):\n",
    "  for epoch in range(num_epochs):\n",
    "    for names, labels in data_loader:\n",
    "      optimizer.zero_grad()\n",
    "      pred, h = model(names, None)\n",
    "      last_output = pred[:, -1, :]\n",
    "      last_output = transform_prediction(last_output)\n",
    "      \n",
    "      loss = criterion(last_output, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    # Print training progress (optional)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Example usage (replace with your data loaders)\n",
    "input_size = len_alphabet\n",
    "hidden_size = 5\n",
    "output_size = len(languages)\n",
    "model = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  \n",
    "\n",
    "train(model, train_loader, criterion, optimizer, num_epochs=100)"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 56\u001B[0m\n\u001B[1;32m     53\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m     54\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)  \n\u001B[0;32m---> 56\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[45], line 41\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, data_loader, criterion, optimizer, num_epochs)\u001B[0m\n\u001B[1;32m     38\u001B[0m   last_output \u001B[38;5;241m=\u001B[39m transform_prediction(last_output)\n\u001B[1;32m     40\u001B[0m   loss \u001B[38;5;241m=\u001B[39m criterion(last_output, labels)\n\u001B[0;32m---> 41\u001B[0m   \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m   optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Print training progress (optional)\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/TSM_DeLearn-xa9RRjL_/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Model with several SimpleRNN Layers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "### START YOUR CODE\n",
    "\n",
    "model = ...\n",
    "\n",
    "### END YOUR CODE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### START YOUR CODE\n",
    "\n",
    "# train...\n",
    "\n",
    "### END YOUR CODE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Handling\n",
    "\n",
    "Choose a method to address the class imbalance seen in the given example.\n",
    "- minority resampling \n",
    "- class weights in the loss\n",
    "\n",
    "Implement it and incorporate it in the training.\n",
    "Evaluate the results and compare it with the results obtained with the unbalanced training.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### START YOUR CODE\n",
    "\n",
    "# train...\n",
    "\n",
    "### END YOUR CODE"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
